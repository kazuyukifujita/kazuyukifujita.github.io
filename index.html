<!DOCTYPE html>
<html lang="en">
	<head>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-122737981-2"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-122737981-2');
        </script>

		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
		<meta name="description" content="">
		<meta name="author" content="">
		<link rel="icon" type="image/png" href="images/logo.png">
		<title>Kazuyuki FUJITA</title>
		<!-- Bootstrap core CSS -->
		<link href="css/bootstrap.min.css" rel="stylesheet">
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
        <link href="https://fonts.googleapis.com/css?family=Lato:400,700|Noto+Sans+JP:400,700" rel="stylesheet">
		<!-- Custom styles for this template -->
		<link href="css/style.css" rel="stylesheet">
	</head>
	<body id="page-top">
		<!-- Navigation -->
		<nav class="navbar navbar-default">
			<div class="container">
				<!-- Brand and toggle get grouped for better mobile display -->
				<div class="navbar-header page-scroll">
					<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
					<span class="sr-only">Toggle navigation</span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					</button>
					<h1><a class="navbar-brand page-scroll" href="#page-top"><img src="images/logo.png" height="36" alt="Kazuyuki FUJITA"></a>Kazuyuki FUJITA</h1>

				</div>
				<!-- Collect the nav links, forms, and other content for toggling -->
				<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
					<ul class="nav navbar-nav navbar-right">
						<li class="hidden">
							<a href="#page-top"></a>
						</li>
						<li>
							<a class="page-scroll" href="#about">About me</a>
						</li>
						<li>
							<a class="page-scroll" href="#research">Reseach</a>
						</li>
						<li>
							<a class="page-scroll" href="#publications">Publications</a>
						</li>
					</ul>
				</div>
				<!-- /.navbar-collapse -->
			</div>
			<!-- /.container-fluid -->
		</nav>

		<section id="about">
			<div class="container">
			<div class="row">
				<div class="col-lg-12 text-center">
					<div class="section-title">
            <h2>About me</h2>
					</div>
            <figure class="portofolio"><img src="images/fujita_photo2.png" alt="Kazuyuki FUJITA 藤田和之"></figure>
            <h3 class="about">
              藤田 和之 / Kazuyuki FUJITA
						</h3>
						<p class="text-left">
							東北大学　電気通信研究所　助教<br />
							Assistant Prof., Research Institute of Electrical Communication, Tohoku University
						</p>
						<p class="text-left">
							<a class="scholar" href="https://scholar.google.co.jp/citations?user=GEglw3kAAAAJ">Google Scholar</a> /
							<a class="researchmap" href="https://researchmap.jp/kazuyukifujita/">researchmap</a> /
							<a class="twitter" href="https://twitter.com/kekure">Twitter</a>
						</p>
						<p class="text-left">
							<a class="icd" href="https://www.icd.riec.tohoku.ac.jp/">ICD Lab.（北村・高嶋研究室）</a>
						</p>
						<p class="text-left">
							Contact: k-fujita [at] riec.tohoku.ac.jp
						</p>
						<h3 class="about">
							Short bio:
						</h3>
						<p class="text-left">
							2010年 大阪大学 大学院情報科学研究科 マルチメディア工学専攻 博士前期課程 修了．2013年 同研究科 情報システム工学専攻博士 後期課程修了．同年 株式会社イトーキ．2018年より東北大学 電気通信研究所 助教（2023年より東北大学プロミネントリサーチフェロー）．博士 (情報科学)．ヒューマンコンピュータインタラクションおよびバーチャルリアリティに関する研究，より具体的には，人の活動を取り巻く「ワークスペース」に着目し，空間全体またはその構成要素とのインタラクションを通して人のさまざまな活動を支援する「ヒューマン・ワークスペース・インタラクション」の研究に取り組んでいます．
						</p>
						<p class="text-left">
							I, Kazuyuki FUJITA, am an Assistant Professor of Research Institute of Electrical Communication (RIEC) at Tohoku University.
              I received my Ph.D. in Information Science and Technology from Osaka University in 2013. I worked for ITOKI, an office space design company, and was engaged in research and development on future offices for 2013-2018. Currently I am working as an Assistant Professor of RIEC at Tohoku University from 2018, and have been granted the title of Prominent Research Fellow at Tohoku University from 2023. My research interests include human-computer interaction and virtual reality. More specifically, I focus on "workspaces" that surrounds human activities, and research <i>Human-Workspace Interaction</i>, which supports various human activities through interaction with the computerized or virtualized workspaces and their components.
            </p>
						</div>


				</div>
			</div>

			</div><!-- container -->
		</section>

		<section id="research">
			<div class="container">
				<div class="row">
					<div class="col-lg-12 text-center">
						<div class="section-title">
							<h2>Research</h2>
						</div>
          </div>
				</div>

				<div class="row">
					<div class="team-item">
						<div class="column1">
								<img src="images/research/visual-auditory-redirection.png" class="img-responsive" alt="Visual-Auditory Redirection">
						</div>
						<div class="column2">
							<h3>Visual-Auditory Redirection using Real-World Auditory Cues</h3>
							<p>
								本研究では，現実空間から発せられる音手がかり（例：テレビの音，洗濯機の音）が，VR空間内でのリダイレクテッドウォーキング（体を回転させたときの視覚のズレ）への知覚に与える影響について調査しました．実験の結果，現実空間内の特定位置に固定された音源を提示した場合，リダイレクテッドウォーキングの効果が高まる（視覚のズレに気づきにくくなる）可能性が示されました．
							</p>
							<p>
								This study investigated the effect of sound cues emitted from the real space (e.g., the sound of a TV and a washing machine) on the perception of redirected walking (visual manipulation when the body is rotated) in VR. The experimental results suggested potential enhancement of redirected walking (i.e., visual manipulation was less noticeable) when a fixed sound source was presented at a specific location in the real space.
							</p>
							<div class="team-position">
								<p><a href="https://www.icd.riec.tohoku.ac.jp/en/research/projects/visual-auditory-redirection">project page</a></p>
								<span class="conference">IEEE TVCG</span><br/>
								<a href="https://doi.org/10.1109/TVCG.2023.3309267">paper</a> / <a href="">video (coming soon)</a>
							</div>
						</div>
					</div>

					<div class="team-item">
						<div class="column1">
								<img src="images/research/HandyGaze.gif" class="img-responsive" alt="HandyGaze">
						</div>
						<div class="column2">
							<h3>HandyGaze</h3>
							<p>
								HandyGazeは，環境内にセンサやマーカを設置せず，スマートフォンを持つだけで利用できる6自由度の視線トラッキング手法です．本手法では，スマートフォンの前面カメラと背面カメラを同時に使用しており，前面カメラでスマートフォンに対するユーザの視線ベクトルの推定を，背面カメラ（および深度センサ）ではあらかじめ取得した環境の3Dマップを復元することで自己位置推定をそれぞれ行うことにより実現しています．この手法は，例えば美術館・博物館での案内アプリケーションへ応用できると期待されます．
							</p>
							<p>
								HandyGaze is a 6-DoF gaze tracking technique that can be carried out by simply holding a smartphone naturally without installing any sensors or markers in the environment. Our technique simultaneously employs the smartphone’s front and rear cameras: The front camera estimates the user’s gaze vector relative to the smartphone, while the rear camera (and depth sensor, if available) performs self-localization by reconstructing a pre-obtained 3D map of the environment. This technique could be used e.g., for gaze-based guidance applications in museums.
							</p>
							<div class="team-position">
								<p><a href="https://www.icd.riec.tohoku.ac.jp/en/research/projects/handygaze/">project page</a></p>
								<span class="conference">Proc. ACM Hum.-Comput. Interact. (ISS 2022)</span><br/>
								<a href="https://doi.org/10.1145/3567715">paper</a> / <a href="https://www.youtube.com/watch?v=Xke7A5JE8D4">video</a>
							</div>
							<div class="team-position">
								<span class="conference">インタラクション2023</span> <em>インタラクティブ発表賞（一般投票）</em><br/>
									paper
							</div>
						</div>
					</div>
					<div class="row">
					<div class="team-item">
						<div class="column1">
								<img src="images/research/TetraForce.gif" class="img-responsive" alt="TetraForce">
						</div>
						<div class="column2">
							<h3>TetraForce</h3>
							<p>
								TetraForceは，スマートフォンの2つの面（タッチ面と背面）に対する2種類の方向（垂直方向と剪断方向）の力による入力を可能にするスマートフォンケース型の磁気式ユーザインタフェースです．本インタフェース，は3DoFで可動する背面パネルに磁石を配置し，その変位をスマートフォン内蔵の地磁気センサにより推定することで力の方向や強さを取得しています．プロトタイプを用いたユーザスタディの結果，ユーザによる4種類の入力が平均97.4%の精度で意図通りに検出できることを確認しました．また，ワークショップでは本インタフェースを用いた多数のアプリケーション例が提案され，応用範囲の広さを示しました．
							</p>
							<p>
								TetraForce is a novel phone-case-shaped interface, which enables four types of force input consisting of two force directions (i.e., pressure force and shear force) and two force-applied surfaces (i.e., touch surface and back surface) in a single device. Force detection is achieved using the smartphone’s built-in magnetometer by estimating the displacement of a magnet attached to a 3-DoF passively movable panel at the back. Our user study demonstrated that the input using the interface was detected as intended by the participants with a success rate of 97.4% on average for all four input types. An ideation workshop also derived a variety of useful application ideas.
							</p>
							<div class="team-position">
								<p><a href="https://www.icd.riec.tohoku.ac.jp/en/research/projects/tetraforce/">project page</a></p>
								<span class="conference">Proc. ACM Hum.-Comput. Interact. (ISS 2022)</span><br/>
								<a href="https://doi.org/10.1145/3567717">paper</a> / <a href="https://www.youtube.com/watch?v=eFlVH5IJWSA&t=1s">presentation video</a><br/>
							</div>
							<div class="team-position">
								<span class="conference">WISS 2022</span><br/>
								<a href="https://www.wiss.org/WISS2022Proceedings/data/01.pdf">paper</a> / <a href="https://www.wiss.org/WISS2022Proceedings/data/01.mp4">video</a><br/>
							</div>
						</div>
					</div>

					<div class="team-item">
						<div class="column1">
								<img src="images/research/WaddleWalls.png" class="img-responsive" alt="WaddleWalls">
						</div>
						<div class="column2">
							<h3>WaddleWalls</h3>
							<p>
								WaddleWallsは，オフィス等の作業スペースにおける視覚的な開放性を状況に応じて調節するために，移動または高さ・幅の変更が可能な自走型パーティションシステムです．本研究では，上下昇降スタンド，ロールスクリーン，移動ロボットを用いてWaddleWallsのプロトタイプを実装し，これを用いた応用シナリオの例を示しました．
							</p>
							<p>
								WaddleWalls is a self-actuated stretchable partition system whose physical height, width, and position can dynamically change to regulate openness of the workplace. We implemented the prototype with a height-adjustable stand, a roll-up screen, and a mobile robot. We then show some example application scenarios to discuss potential future actuated-territorial offices.
							</p>
							<div class="team-position">
								<p><a href="https://www.icd.riec.tohoku.ac.jp/en/research/projects/waddlewalls/">project page</a></p>
								<span class="conference">UIST 2022</span><br/>
								<a href="https://doi.org/10.1145/3526113.3545615">paper</a> / <a href="https://youtu.be/LCCvpnw1tEw?si=AKS3-tD0BCB-HEQ_">video</a>
							</div>
						</div>
					</div>

					<div class="team-item">
						<div class="column1">
								<img src="images/research/confusionlens.gif" class="img-responsive" alt="ConfusionLens">
						</div>
						<div class="column2">
							<h3>ConfusionLens</h3>
							<p>
								ConfusionLensは，機械学習による画像分類結果をインタラクティブに可視化するユーザインタフェースです．従来の混同行列にFocus+Contextによる可視化を適用することで，1画面上で全てのインスタンス画像を表示しながら，タスクに合わせて表示レイアウトをシームレスに切り替えることを可能としました．初期プロトタイプとして，数百のインスタンスをサポートするインタフェースに加え，インスタンスのソート，フィルタリング，活性化マップの可視化といった実用的な拡張機能を実装しました．
							</p>
							<p>
								ConfusionLens is a dynamic and interactive visualization interface that augments a conventional confusion matrix with focus+context visualization. This interface allows users to seamlessly switch table layouts among different views while observing all of the instance images in a single screen. Our initial prototype provides a user interface that supports hundreds of instances and its several practical extensions such as activation map visualization and instance sorting/filtering.
							</p>
							<div class="team-position">
								<span class="conference">UIST 2022 Demo</span><br/>
								<a href="https://doi.org/10.1145/3526114.3558631">paper</a> / <a href="https://youtu.be/tgvqFhoGfco">video</a>
							</div>
						</div>
					</div>
					<div class="team-item">
						<div class="column1">
								<img src="images/research/redirecteddoors.gif" class="img-responsive" alt="RedirectedDoors">
						</div>
						<div class="column2">
							<h3>RedirectedDoors</h3>
							<p>
								RedirectedDoorsは，VR空間内でドアを開ける動作中にユーザの進行方向を操作するリダイレクション手法です（リダイレクション手法とは，歩行を伴うVR体験を限られた物理空間内で楽しむために，ユーザに気づかれないように移動量や方向をだますことで物理空間を有効活用しようとする手法のことをいいます）．具体的には，開扉時のユーザとドアの間の位置関係を保ちながら，開扉角度に合わせてVR空間全体を一定の倍率で回転させることにより，ユーザの進行方向操作を実現しています．
							</p>
							<p>
								RedirectedDoors is a novel space-efficient technique for redirection in VR focused on door-opening behavior (Redirection is a methodology to subtly manipulate the user's movement distance and/or direction for compressing the VR experience within a limited physical space). This technique manipulates the user’s walking direction by rotating the entire virtual environment at a certain angular ratio of the door being opened, while the virtual door’s position is kept unmanipulated so that a realistic door-opening user experience can be ensured.
							</p>
							<div class="team-position">
								<p><a href="https://www.icd.riec.tohoku.ac.jp/en/research/projects/redirecteddoors/">project page</a></p>
								<span class="conference">IEEEVR 2022 Conference paper</span> <em>Honorable Mention Award</em><br/>
								<a href="https://ieeexplore.ieee.org/document/9756802">paper</a> / <a href="https://youtu.be/Iv-1pdqwahQ">video</a>
							</div>
							<div class="team-position">
								<span class="conference">第26回 VR学会大会 </span> <br/>
								<a href="http://conference.vrsj.org/ac2021/program/doc/2B3-7.pdf">paper</a>
							</div>
						</div>
					</div>
					<div class="team-item">
						<div class="column1">
								<img src="images/research/pseudojumpon.gif" class="img-responsive" alt="PseudoJumpOn">
						</div>
						<div class="column2">
							<h3>PseudoJumpOn</h3>
							<p>
								PseudoJumpOnは，平坦な場所でもVR空間内で擬似的に段差に跳び乗ったような感覚を与える手法です．この手法は，跳躍動作中の上昇量と下降量に異なる倍率をかけるゲイン操作手法と，上昇時間が下降時間よりも長くなるように操作するピークシフト手法を組み合わせることにより実現されています．ユーザスタディ（N=20）の結果，ユーザは物理的な段差がないことを知りながらも，段差に飛び乗る感覚のリアリティや自然さに関して概ねポジティブな評価であったことがわかりました．
							</p>
							<p>
								PseudoJumpOn is a novel locomotion technique using a common VR setup that allows the user to experience virtual step-up jumping motion. This technique is achieved by combining two viewpoint-manipulation methods: gain manipulation, which differentiates the ascent and descent height, and peak shifting, which delays the peak timing. Results of a user study (N=20) showed that the users in most conditions felt positively in terms of reality and naturalness of actually jumping onto steps, even though knowing no physical steps existed.
							</p>
							<div class="team-position">
								<p><a href="https://www.icd.riec.tohoku.ac.jp/en/research/projects/pseudojumpon/">project page</a></p>
								<span class="conference">IEEEVR 2022 Conference paper</span><br/>
								<a href="https://ieeexplore.ieee.org/document/9756765">paper</a> / <a href="https://youtu.be/4QTR0L5VTdE">video</a>
							</div>
							<div class="team-position">
								<span class="conference">第26回 VR学会大会 </span> <br/>
								<a href="http://conference.vrsj.org/ac2021/program/doc/2D3-3.pdf">paper</a>
							</div>
						</div>
					</div>
					<div class="team-item">
						<div class="column1">
								<img src="images/research/balancing.png" class="img-responsive" alt="Towards Balancing VR Immersion and Bystander Awareness">
						</div>
						<div class="column2">
							<h3>Towards Balancing VR Immersion and Bystander Awareness</h3>
							<p>
								ヘッドマウントディスプレイ（HMD）ユーザがVR体験への没入と周囲へのアウェアネスのバランスをうまくとる方法を探るため，近接者の位置と向きをHMDの視界に可視化する3つの手法（Avatar View, Radar, and Presence++）を設計し実装しました．3つの異なるタイプのVRコンテンツを用いたユーザスタディ（N=20）の結果，近接者のアバタをVR空間内に半透明表示する手法であるAvatar Viewがアウェアネスの向上に有効であったものの，VRコンテンツへの没入感をやや低下させることがわかりました．この結果を踏まえ，HMDユーザの没入感の低下を緩和するために今後どのような可視化技術を設計すべきか考察しました．
							</p>
							<p>
								To explore how to balance VR immersion and bystander awareness for HMD users, we adapted and implemented three visualization techniques (Avatar View, Radar, and Presence++) that share bystanders' location and orientation with headset users. Our user study (N=20) with three different types of VR content (high, medium, low interactivity) reveals that a see-through avatar representation of bystanders was effective, but led to slightly reduced immersion in the VR content. Based on our findings, we discuss how future awareness visualization techniques can be designed to mitigate the reduction of immersion for the headset user.
							</p>
							<div class="team-position">
								<span class="conference">Proc. ACM Hum.-Comput. Interact. (ISS 2021)</span> <em>Best Paper Award</em><br/>
																	<a href="https://dl.acm.org/doi/10.1145/3486950">paper</a>
							</div>
						</div>
					</div>
					<div class="team-item">
						<div class="column1">
								<img src="images/research/modularhmd.png" class="img-responsive" alt="ModularHMD">
						</div>
						<div class="column2">
							<h3>ModularHMD</h3>
							<p>
								ModularHMDは，手動で再構成可能なモジュラー機構を用いた新たなモバイルヘッドマウントディスプレイ（HMD）です．この HMD では，市販の HMD のフェイスカバー部に 3 つの着脱可能なディスプレイモジュールを設置することで，没入感の高い VR 体験を可能にしつつも，必要に応じてモジュールの着脱により視界周辺部での実物体や近接者とのアドホックなインタラクションを可能にします．
							</p>
							<p>
								ModularHMD is a new mobile head-mounted display concept, which allows a user to perform ad-hoc peripheral interaction with real-world devices or people during VR experiences. The device is comprised of a central HMD and three removable modules installed in the periphery of the HMD cowl and each module has four main states: occluding, extended VR view, video see-through (VST), and removed/reused. A user can quickly setup the HMD forms, functions, and real-world visions for ad-hoc peripheral interactions as needed.
							</p>
							<div class="team-position">
								<span class="conference">UIST 2021</span><br/>
                                <a href="https://doi.org/10.1145/3472749.3474738">paper</a> / <a href="https://youtu.be/zVrEkRfo42Y">video</a> / <a href="https://youtu.be/a3TxeANQQVg">presentation video</a>
							</div>
							<div class="team-position">
								<span class="conference">インタラクション 2021</span> <em>インタラクティブ発表賞（PC推薦）</em><br/>
								<a href="https://www.interaction-ipsj.org/proceedings/2021/data/pdf/2A03.pdf">paper</a>
							</div>
						</div>
					</div>
					<div class="team-item">
						<div class="column1">
								<img src="images/research/tiltchair.png" class="img-responsive" alt="TiltChair">
						</div>
						<div class="column2">
							<h3>TiltChair</h3>
							<p>
								TiltChairは，デスクワークにおける長時間の座位を解消するため，オフィスチェア座面を前傾させることでユーザの姿勢を誘導するシステムです．空気圧制御を用い，前傾角度とその動作速度を調整することにより，ユーザが遂行中のタスクを邪魔せずに立位姿勢を促せることを示しました．
							</p>
							<p>
								We propose TiltChair, an actuated office chair that physically manipulates the user’s posture by actively inclining the chair’s seat to address problems associated with prolonged sitting. The system controls the inclination angle and motion speed with the aim of achieving manipulative but unobtrusive posture guidance. Experimental results show the system's potential to change the users posture even during tasks, without loss of task performance.
							</p>
							<p><a href="https://www.icd.riec.tohoku.ac.jp/en/research/projects/tiltchair/">project page</a></p>
							<div class="team-position">
								<span class="conference">CHI 2021</span><br/>
                                <a href="https://doi.org/10.1145/3411764.3445151">paper</a> / <a href="https://youtu.be/9XvbA-nKTBc">video</a> / <a href="https://youtu.be/EuBmw3hVEuM">presentation video</a>
							</div>
							<div class="team-position">
								<span class="conference">インタラクション 2021</span><br/>
								<a href="https://www.interaction-ipsj.org/proceedings/2021/data/pdf/1A07.pdf">paper</a>
							</div>
						</div>
					</div>
					<div class="team-item">
						<div class="column1">
								<img src="images/research/pinpointfly.png" class="img-responsive" alt="PinpointFly">
						</div>
						<div class="column2">
							<h3>PinpointFly</h3>
							<p>
								PinpointFlyは，モバイルARを用いることで，タッチ操作でドローンの位置制御が可能な操縦インターフェイスです．ユーザは，画面に表示されたドローンのバーチャルな影やスライダバーを用いて，ドローンの位置・方向・高さを制御することできます．実験により，この手法が従来のジョイスティック操作に比べて早く，正確に，少ない負荷でドローンの制御を実現することを示しました．
							</p>
							<p>
								We propose PinpointFly, an egocentric interface that allows users to arbitrarily position and rotate a flying drone by position control interactions on a see-through mobile AR where the drone position and its direction are visually enhanced with a virtual cast-shadow. A user study demonstrated that PinpointFly makes the drone positioning and inspection operations faster, more accurate, simpler and fewer workload than a conventional joystick interface.
							</p>
							<p><a href="https://www.icd.riec.tohoku.ac.jp/en/research/projects/pinpointfly/">project page</a></p>
							<div class="team-position">
								<span class="conference">CHI 2021</span><br/>
                                <a href="https://dl.acm.org/doi/10.1145/3411764.3445110">paper</a> / <a href="https://youtu.be/G9H4TvE3mVE">video</a> / <a href="https://youtu.be/A1yZ2PVTtTM">presentation video</a>
							</div>
							<div class="team-position">
								<span class="conference">SIGGRAPH Asia 2019 E-Tech</span><br/>
								<a href="https://dl.acm.org/doi/10.1145/3355049.3360534">paper</a>
							</div>
						</div>
					</div>
					<!-- team member item -->					<!-- team member item -->
					<!-- team member item -->					<!-- team member item -->
					<div class="team-item">
						<div class="column1">
								<img src="images/research/bouncyscreen.png" class="img-responsive" alt="BouncyScreen">
						</div>
						<div class="column2">
							<h3>BouncyScreen</h3>
							<p>
								BouncyScreenは，ユーザが平面ディスプレイに表示された物体と間接インタラクションする際，映像と連動して平面ディスプレイを物理的に奥行き方向へ運動させることで疑似触覚を提示することでコンテンツの表現を拡張する手法です．心理物理実験の結果，ディスプレイ自体の物理的な移動は，平面ディスプレイ内に表示された視覚的なコンテンツの移動と同程度に奥行方向への近くに影響を与えることがわかりました．
							</p>
							<p>
								We propose BouncyScreen, an actuated 1D display that enriches indirect interaction with a virtual object by pseudo-haptic feedback mechanics enhanced through the screen's physical movements. The psychophysical study confirmed that the screen's synchronous physical motions offers identical pseudo-force feedback to the vision-based pseudo-haptic technique.
							</p>
							<p><a href="https://www.icd.riec.tohoku.ac.jp/en/research/projects/bouncyscreen/">project page</a></p>
							<div class="team-position">
								<span class="conference">IEEEVR 2021 Conference paper</span><br/>
                                <a href="https://doi.org/10.1109/VR50410.2021.00059">paper</a> / <a href="https://youtu.be/fDWvzDFSZds">video</a>
							</div>
							<div class="team-position">
								<span class="conference">第24回 日本VR学会大会</span><br/>
								<a href="http://conference.vrsj.org/ac2019/program/common/doc/pdf/1A-07.pdf">paper</a>
							</div>
						</div>
					</div>
					<!-- team member item -->					<!-- team member item -->
					<div class="team-item">
						<div class="column1">
								<img src="images/research/zoomwall.png" class="img-responsive" alt="ZoomWalls">
						</div>
						<div class="column2">
							<h3>ZoomWalls</h3>
							<p>
								ルームスケールVRにおける壁やドア等の空間インフラに関する触覚を，複数の自走する壁型プロップを用いてユーザに提示する方法であるZoomWallを提案しています．本研究では，ユーザの動きから触れそうな壁を予測するアルゴリズム，および，その場所へ利用可能な壁面プロップを配置するための経路計画アルゴリズムを設計し，ユーザスタディによりこの有効性を検証しています．
							</p>
							<p>
								We propose ZoomWalls, autonomous robotic encounter-type wall-shaped props that simulate the haptic infrastructure (i.e. walls, doors) in room-scale virtual reality. Based on a user's movement through the physical space, ZoomWall props are coordinated through a predict-and-dispatch architecture to provide just-in-time haptic feedback for objects the user is about to touch.
							</p>
							<p><a href="https://www.icd.riec.tohoku.ac.jp/en/research/projects/zoomwalls/">project page</a></p>
							<div class="team-position">
								<span class="conference">UIST 2020</span><br/>
								<a href="https://doi.org/10.1145/3379337.3415859">paper</a> / <a href="https://youtu.be/j2iSNDkBxAY">video</a> / <a href="https://youtu.be/jHigrvoDiEg">presentation video</a>
							</div>
							<div class="team-position">
								<span class="conference">インタラクション 2020</span><br/>
								<a href="https://www.interaction-ipsj.org/proceedings/2020/data/pdf/INT20014.pdf">paper</a>
							</div>
						</div>
					</div>
					<!-- team member item -->
					<div class="team-item">
						<div class="column1">
								<img src="images/research/plant.png" class="img-responsive" alt="StickyTouch">
						</div>
						<div class="column2">
							<h3>PlanT</h3>
							<p>
								本研究では，植物の成長度合いを制御することにより，日々蓄積されていく情報（例えば，タスクの進捗状況や貯金箱の金額など）を可視化する植物ディスプレイ「PlanT」を提案しています．実験では，カイワレ大根に与える水分や日光の量の変化により，主観的な成長度合いを制御できる可能性を示しています．
							</p>
							<p>
								We propose PlanT, a plant display that visualizes information accumulating over time (e.g., task progress, piggy bank amounts, etc.) by controlling the growth rate of the plant. Our experiments show the possibility to control the growth rate (perceived by users) of white radish sprouts by changing the amount of water and sunlight provided to them.
							</p>
							<div class="team-position">
								<span class="conference">インタラクション 2020</span><br/>
								<a href="https://www.interaction-ipsj.org/proceedings/2020/data/pdf/INT20003.pdf">paper</a>
								<div class="team-position">・2020/11/04 ITmedia NEWSで紹介されました．<br />
								<a href="https://www.itmedia.co.jp/news/articles/2011/04/news065.html">タスクの進捗に応じて成長する“植物ディスプレイ”、やる気向上にも　阪大と東北大「PlanT」開発</a></div>
							</div>
						</div>
					</div>
					<!-- team member item -->
					<div class="team-item">
						<div class="column1">
							<img src="images/research/graphviz.png" class="img-responsive" alt="Techniques to Visualize Occluded Graph Elements for 2.5D Map Editing">
						</div>
						<div class="column2">
							<h3>Visualization techniques for 2.5D Map Editing</h3>
							<p>
								複数階層にわたる2.5D地図情報のグラフデータを編集するための，グラフ要素のオクルージョン（遮蔽）を解決するユーザインタフェースを提案しています．提案手法Repel Significationは，オクルージョン箇所を振動するようなモーションで可視化します．2つ目の提案手法Repel Expansionは，カーソル付近のオクルージョンを解決するように，重なっていた要素を出現させ可視化します．
							</p>
							<p>
								We propose an interface to visualize occluded graph elements that help the user edit map data with a 2.5D geographical structure (e.g., multi-floor indoor maps). The proposed technique named Repel Signification employs oscillating motion to signify the occluded elements. The other technique named Expansion Interaction visualizes occluded elements around cursor.
							</p>
							<div class="team-position">
								<span class="conference">CHI 2020 LDW</span><br/>
								<a href="https://dl.acm.org/doi/abs/10.1145/3334480.3382987">paper</a> / <a href="https://youtu.be/v57WGBsO9b8">video</a>
							</div>
							<div class="team-position">
								<span class="conference">インタラクション 2020</span><br/>
								<a href="https://www.interaction-ipsj.org/proceedings/2020/data/pdf/2B-41.pdf">paper</a>
							</div>
						</div>
					</div>
					<!-- end team member item -->
					<!-- team member item -->
					<div class="team-item">
						<div class="column1">
								<img src="images/research/StickyTouch.jpg" class="img-responsive" alt="StickyTouch">
						</div>
						<div class="column2">
							<h3>StickyTouch</h3>
							<p>
								StickyTouchは，表面の一部の粘着性をコンピュータによって制御でき，画像情報（2D）に粘着性（1D）を追加し表示できるディスプレイです．温度によって粘着性が変化する特殊なポリマーシートをディスプレイスクリーン上に配置し，局所的な温度変化を制御することで，表面の粘着性を局所的に制御することを実現しています．
							</p>
							<p>
								StickyTouch is a novel tactile display that represents adhesive information on a surface. Adhesion control can be achieved by a temperature sensitive adhesive sheet whose temperature is locally controlled by peltier devices arranged in a grid.
							</p>
							<div class="team-position"><span class="conference">SIGGRAPH Asia 2019 E-Tech</span> <em>Best Demo Voted by Committee</em><br/>
							<a href="https://dl.acm.org/doi/10.1145/3355049.3360531">paper</a> / <a href="https://youtu.be/05rpa3_XECU">video</a></div>
							<div class="team-position"><span class="conference">IEEE Haptics 2020</span><br/>
							<a href="https://doi.org/10.1109/HAPTICS45997.2020.ras.HAP20.4.b5d4a51b">paper</a></div>
							<div class="team-position"><span class="conference">VR学会論文誌</span> <em>日本バーチャルリアリティ学会 第23回論文賞</em><br/>
							<a href="https://doi.org/10.18974/tvrsj.25.4_384">paper</a></div>
							<div class="team-position">・2019/12/18 ITmedia NEWSで紹介されました．<br />
							<a href="https://www.itmedia.co.jp/news/articles/1912/18/news035.html">粘着力でフィードバックする触覚ディスプレイは、納豆も再現可能？　阪大と東北大の「StickyTouch」</a></div>
						</div>
					</div>
					<!-- end team member item -->
					<!-- team member item -->
					<div class="team-item">
						<div class="column1">
								<img src="images/research/PursuitSensing.png" class="img-responsive" alt="Pursuit Sensing">
						</div>
						<div class="column2">
							<h3>Pursuit Sensing</h3>
							<p>
								Pursuit Sensingは，カメラによるハンドトラッキング可能な領域を拡大するために，3軸ジンバルを用いてカメラアングルを手の動きに追従させるセンシング手法です．アクションカメラ用3軸ジンバルとLeap Motionセンサーを組み合わせたウェアラブルなセンサを胸部に固定し，VRでのハンドインタラクションを実現するアプリケーションを開発しました．
							</p>
							<p>
								Pursuit Sensing is a technique to considerably extend the tracking volume of a camera sensor through self-actuated reorientation using a customized gimbal, thus enabling a Leap Motion to dynamically follow the user’s hand position in mobile HMD scenarios.
							</p>
							<div class="team-position"><span class="conference">SUI 2019</span><br/>
							<a href="https://dl.acm.org/doi/10.1145/3357251.3357578">paper</a> / <a href="https://youtu.be/0-sKlZGyhII">video</a></div>
							<div class="team-position">・2020/1/9 ITmedia NEWSで紹介されました．<br />
							<a href="https://www.itmedia.co.jp/news/articles/2001/09/news035.html">VRのハンドトラッキング範囲を拡張する「Pursuit Sensing」　東北大学など開発</a></div>
						</div>
					</div>
					<!-- end team member item -->
          <!-- team member item -->
					<div class="team-item">
						<div class="column1">
								<img src="images/research/shearsheet.png" class="img-responsive" alt="ShearSheet">
						</div>
						<div class="column2">
							<h3>ShearSheet</h3>
							<p>
								ShearSheetは，導電性素材を貼付した透明シートをタッチスクリーン上でスライドさせることで，通常のタッチ入力に影響を与えずに，ジョイスティックのような速度制御による機構を追加のセンサ無しで付加する新たなインタフェースです．
							</p>
							<p>
								ShearSheet is a low-cost and power-free method that enables tangential (shear) force input on a touchscreen using a rubber-mounted slim transparent sheet. The sheet has tiny conductive material(s) attached to the underside so that displacement of the sheet is recognized as touch input(s).
							</p>
							<div class="team-position"><span class="conference">ISS 2019</span> <em>Best Demo Award</em><br/>
							<a href="https://dl.acm.org/citation.cfm?id=3359717">paper</a> / <a href="https://youtu.be/17sIGr6DT78">video</a></div>
							<div class="team-position"><span class="conference">インタラクション2019</span> <em>インタラクティブ発表賞（PC推薦）</em><br/>
							<a href="http://www.interaction-ipsj.org/proceedings/2019/data/pdf/1B-25.pdf">paper</a></div>
							<div class="team-position"><span class="conference">情報処理学会論文誌</span><br/>
							<a href="http://id.nii.ac.jp/1001/00209337/">paper</a></div>
              <div class="team-position">・2019/11/27 ITmedia NEWSで紹介されました．<br />
              <a href="https://www.itmedia.co.jp/news/articles/1911/27/news042.html">タッチパネルに透明シートで、ジョイスティック的な操作可能に　東北大と芝浦工大が開発</a></div>
						</div>
					</div>
					<!-- end team member item -->
				    <!-- team member item -->
					<div class="team-item">
						<div class="column1">
								<img src="images/research/TPP.png" class="img-responsive" alt="Third-Person Piloting">
						</div>
						<div class="column2">
							<h3>Third-Person Piloting</h3>
							<p>
								本インタフェースは，空間的に連動する2台のドローンを利用して従来のドローン操縦インタフェースを拡張します．操縦対象のドローン（主ドローン）カメラによる一人称視点に加えて，主ドローンに空間連動する副ドローンを用いて広域な三人称視点を提供することで，パイロットのドローン周囲への理解（SituationalAwareness）を高め，ドローンの操縦や飛行経路計画をより簡単にします．
							</p>
							<p>
								We propose Third-Person Piloting, a novel drone manipulation interface that increases situational awareness using an interactive third-person perspective from a second, spatially coupled drone. The pilot uses a controller with a manipulatable miniature drone. This allows the pilot to obtain various third-person perspectives by controlling a changing the orientation of the miniature drone while maintaining standard primary drone control using the conventional controller.
							</p>
							<div class="team-position"><span class="conference">UIST 2019</span><br/>
							<a href="http://doi.acm.org/10.1145/3332165.3347953">paper</a> / <a href="https://youtu.be/cQsWZJa5YpM">video</a></div>
							<div class="team-position"><span class="conference">インタラクション2019</span><br/>
							<a href="http://www.interaction-ipsj.org/proceedings/2019/data/pdf/INT19012.pdf">paper</a></div>
							<div class="team-position"><span class="conference">情報処理学会論文誌</span><br/>
							<a href="http://doi.org/10.20729/00206267">paper</a></div>
              <div class="team-position">・2019/11/28 ITmedia NEWSで紹介されました．<br />
              <a href="https://www.itmedia.co.jp/news/articles/1911/28/news035.html">飛行中のドローンを別ドローンで客観視　2画面と模型で操縦を簡単に　東北大など研究</a></div>
						</div>
					</div>
					<!-- end team member item -->
					<!-- team member item -->
					<div class="team-item">
						<div class="column1">
							<img src="images/research/redirectedjump.png" class="img-responsive" alt="Redirected Jumping">
						</div>
						<div class="column2">
							<h3>Redirected Jumping</h3>
							<p>本研究は，VR空間において，ユーザによる跳躍移動量（水平距離，高さ，回転量）を知覚されない程度のゲインにより拡大（または縮小）することで，限られた物理空間内でより自由度の高い跳躍体験を実現する枠組みであるRedirected Jumpingを提案しています．</p>
							<p>We explore Redirected Jumping, a novel redirection technique which enables us to purposefully manipulate the mapping of the user’s physical jumping movements (e.g., distance and direction) to movement in the virtual space, allowing richer and more active physical VR experiences within a limited tracking area.</p>
							<p><a href="https://www.icd.riec.tohoku.ac.jp/en/research/projects/redirected-jumping/">project page</a></p>
							<div class="team-position"><span class="conference">IEEE VR 2019 Conference paper</span><br/>
							<a href="https://ieeexplore.ieee.org/document/8797989/">paper</a> / <a href="https://youtu.be/kR9YI4kdgJI">video</a></div>
							<div class="team-position"><span class="conference">VR学会論文誌</span> <em>日本バーチャルリアリティ学会 第22回論文賞</em><br/>
							<a href="https://doi.org/10.18974/tvrsj.24.4_341">paper</a></div>
						</div>
					</div>
					<!-- end team member item -->
					<!-- team member item -->
						<div class="team-item">
							<div class="column1">
								<img src="images/research/dflip.jpg" class="img-responsive" alt="D-FLIP">
							</div>
							<div class="column2">
								<h3>D-FLIP</h3>
								<p>
									デジタル写真を閲覧する機会が増えていますが，写真の数が多くなるにつれ，それらの関連付けを考慮することは難しくなります．本研究では，写真のメタデータを利用しさまざまなコンテンツを状況に応じて動的に，またインタラクティブに表示する新しい手法を提案しています.
								</p>
								<p>
									D-FLIP dynamically and flexibly visualizes digital photos and their metadata. We design various dynamic photo visualizations with up to four-dimensional meta-information, allowing users to dynamically and effectively manage photos by selecting meta-information of user's current needs and interest.
								</p>
								<p>
									<a href="https://www.icd.riec.tohoku.ac.jp/en/research/projects/Interactive-content-visualization/">project page</a>
								</p>
								<div class="team-position"><span class="conference">ISS 2018 demo</span><br/>
								<a href="https://doi.org/10.1145/3279778.3279923">paper</a> / <a href="https://youtu.be/dm1i0dFCiyI">video</a></div>
							</div>
						</div>
					<!-- end team member item -->
					<!-- team member item -->
					<div class="team-item">
						<div class="column1">
							<img src="images/research/ai_meeting.jpg" class="img-responsive" alt="AI-Supported Meeting Space">
						</div>
						<div class="column2">
							<div class="team-text">
								<h3>AI-Supported Meeting Space</h3>
								<p>
									オフィスの会議において，参加者の発想支援や合意形成を目的とし，空間自体が「もうひとりの参加者」となり会議を支援する空間型インタフェースを提案しています．参加者の発話内容に関連する情報を，壁面にリアルタイムに表出させる仕組みを実際の会議室に実装し，その有用性を検証しています．
								</p>
								<p>
									AI-Supported Meeting Space itself behaves as “another participant” to make the meeting more productive. The system periodically provides the participants with some information related to their discussion topics on the surface of the table and walls.
								</p>
								<div class="team-position"><span class="conference">JSAI 2017 (Domestic)</span><br/>
								<a href="https://doi.org/10.11517/pjsai.JSAI2017.0_2P14">paper</a></div>
							</div>
						</div>
					</div>
					<!-- end team member item -->
					<!-- team member item -->
					<div class="team-item">
						<div class="column1">
							<img src="images/research/elastic2.png" class="img-responsive" alt="elastic scroll and zoom">
						</div>
						<div class="column2">
							<h3>Elastic Scroll and Zoom</h3>
							<p>
								紙や布を指で縮めようとしたときに発生する「撓（たわ）み」のメタファを，スクロールおよびズーム操作に適用したインタラクション手法を提案しています．この手法は表示するコンテンツを伸縮性の柔らかい材質と捉えており，スクロール・ズームの際に従来は画面外へ押し出されていた領域が画面内に一時的に留まることにより，元の場所に戻る操作を支援します．
							</p>
							<p>
								Elastic Scroll and Zoom is a viewport control method using metaphor of flexible materials. When a user scrolls or zooms the content to acquire a new area, originally displayed area remains in the viewport with distortion, which helps users decide next operation.
							</p>
							<div class="team-position"><span class="conference">UIST 2012 demo</span><br/>
							<a href="https://doi.org/10.1145/2380296.2380307">paper</a> / <a href="https://youtu.be/bW6rc7RDUwY">video</a></p></div>
							<div class="team-position"><span class="conference">インタラクション2011</span><br/>
							<a href="http://www.interaction-ipsj.org/archives/paper2011/oral/0019/0019.pdf">paper</a></div>
						</div>
					</div>
					<!-- end team member item -->
					<!-- team member item -->
					<div class="team-item">
						<div class="column1">
							<img src="images/research/paranga.png" class="img-responsive" alt="Paranga">
						</div>
						<div class="column2">
							<h3>Paranga</h3>
                            <p>
                                パラパラ漫画を電子的に楽しむことを可能にする本型デバイス「パランガ」では，本を曲げたり親指でページを押さえたりすることで連続的なページめくりを入力し，それに伴い紙のページがめくれる触覚・聴覚フィードバックを実現しています．また，ページめくりの速度によってコンテンツが変わるインスタレーションを実装し，新たな電子パラパラ漫画体験を実現しました．
                            </p>
							<p>
								Paranga is a book-shaped device that brings e-books into physical features like paper-like texture and page-flipping sensation. Paranga detects how quickly a user is turning pages and provides the tactile feedback of turning pages on his/her thumb by employing a rotatable roller mechanism with pieces of real paper.
							</p>

							<div class="team-position"><span class="conference">ACE 2012</span><br/>
							<a href="https://doi.org/10.1007/978-3-642-34292-9_2">paper</a> / <a href="https://youtu.be/NqN9Y0zBEKk">video</a></div>
							<div class="team-position"><span class="conference">SIGGRAPH Asia 2011 E-Tech</span><br />
                            <a href="https://dl.acm.org/citation.cfm?id=2073380">paper</a></div>
                            <div class="team-position"><span class="conference">VR学会論文誌</span><br/>
                            <a href="https://doi.org/10.18974/tvrsj.19.4_477">paper</a></div>
						</div>
					</div>
					<!-- end team member item -->
					<!-- team member item -->
					<div class="team-item">
						<div class="column1">
							<img src="images/research/ambientsuite.jpg" class="img-responsive" alt="Ambient Suite">
						</div>
						<div class="column2">
							<h3>Ambient Suite</h3>
							<p>
                                多人数でのコミュニケーションを支援するための情報環境Ambient Suiteは，マイクやモーショントラッカ等を用いて会話の状況を推定するとともに，床や壁に配置されたディスプレイ群から様々な情報を表示させることで，会話の活性化を促します．101名の参加者による評価実験の結果，これらの情報提示が参加者の会話を活性化したことを示しました．
                            </p>
                            <p>
                                Ambient Suite enhances communication among multiple participants. In Ambient Suite itself works as both sensors to estimate the conversation states of participants from nonverbal cues and displays to present information to stimulate conversation.
							</p>
							<div class="team-position"><span class="conference">ACE 2011</span><br/>
							<a href="https://doi.org/10.1145/2071423.2071454">paper</a> / <a href="https://youtu.be/RVy2w4KAHeE">video</a></div>
							<div class="team-position"><span class="conference">電子情報通信学会論文誌</span><br/>
							<a href="https://search.ieice.org/bin/summary.php?id=j96-d_1_120">paper</a></div>
						</div>
					</div>
					<!-- end team member item -->
					<!-- team member item -->
					<div class="team-item">
						<div class="column1">
							<img src="images/research/fusa2.jpg" class="img-responsive" alt="FuSA2 Touch Display">
						</div>
						<div class="column2">
							<h3>FuSA<sup>2</sup> Touch display</h3>
							<p>
                                撫でる，かきむしるなど様々な動作を想起させる毛状物体の特性に着目し，毛状表面での映像提示とマルチタッチ認識が可能な大画面毛状ディスプレイFuSA<sup>2</sup> Touch Displayを提案しています．このシステムでは，プラスチック製の光ファイバを用い，プロジェクタによる映像提示と赤外線を利用したタッチ認識を実装しています．
                            </p>
							<p>
								FuSA2 Touch Display is a furry and scalable multi-touch display which affords various interactions such as stroking or clawing. The system utilizing the feature of plastic fiber optics realizes a furry-type texture and a simple configuration that integrates the input and output.
							</p>
							<div class="team-position"><span class="conference">ITS 2011</span><br/>
							<a href="https://doi.org/10.1145/2076354.2076361">paper</a> / <a href="https://youtu.be/-MD-fOYe3AI">video</a></div>
                            <div class="team-position"><span class="conference">SIGGRAPH 2010 E-Tech</span><br />
                            <a href="https://dl.acm.org/citation.cfm?id=1836832">paper</a></div>
                            <div class="team-position"><span class="conference">情報処理学会論文誌</span><br/>
							<a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=repository_uri&item_id=81283">paper</a></div>
						</div>
					</div>
					<!-- end team member item -->
					<!-- team member item -->
					<div class="team-item">
						<div class="column1">
							<img src="images/research/anchorednavi.png" class="img-responsive" alt="Anchored Navigation">
						</div>
						<div class="column2">
							<h3>Anchored Navigation</h3>
                            <p>
                                Googleマップ等の地図を操作するための新たな手法であるAnchored Navigationを提案しています．この手法では，ユーザの決めた注目点が画面内に必ず留まるように，ユーザのドラッグ操作に合わせて自動的にズームやチルトがなされることにより，シンプルな操作で注目点との距離・方向感覚を失わない操作を可能としています．
                            </p>
							<p>
								Anchored Navigation is a novel map navigation technique which allows users to manipulate a viewport without mode-switching among pan, zoom, and tilt while maintaining a sense of distance and direction by coupling users' panning displacements with zooming and panning so that the anchor point (determined by users) always remains in the viewport.
							</p>
							<div class="team-position"><span class="conference">GI 2010</span><br/>
							<a href="https://dl.acm.org/citation.cfm?id=1839255">paper</a> / <a href="https://youtu.be/gLizKRp_-TU">video</a></div>
							<div class="team-position"><span class="conference">電子情報通信学会論文誌</span><br/>
							<a href="https://search.ieice.org/bin/summary.php?id=j93-d_11_2454">paper</a></div>
                        </div>
                    </div>
					<!-- end team member item -->
					<!-- team member item -->
					<div class="team-item">
						<div class="column1">
						  <img src="images/research/funbrella.jpg" class="img-responsive" alt="Funbrella">
						</div>
						<div class="column2">
							<h3>アソブレラ / Funbrella</h3>
                            <p>
                                降雨体験を再現するために傘の手元に伝わる振動触覚に着目し，この振動を記録・再生可能な傘型デバイス「アソブレラ」を提案しています．このデバイスでは，ダイナミックマイクまたはスピーカの原理を利用したシンプルな機構により振動の記録・再生の両方を実現し，雨以外の様々なモノが降り注ぐ体験が可能なインスタレーションを実装しました．
                            </p>
				            <p>
                                Funbrella is an umbrella-like device that entertains people by reproducing various kinds of vibration perceived through an umbrella's handle while raining. We implemented a vibration-giving mechanism with an extremely simple structure based on a microphone / speaker so that the device can record the vibrations and plays them.
							</p>
							<div class="team-position"><span class="conference">ACE 2009</span><br/>
                            <a href="https://doi.org/10.1145/1690388.1690400">paper</a></div>
							<div class="team-position"><span class="conference">SIGGRAPH 2009 E-Tech</span><br />
                            <a href="https://dl.acm.org/citation.cfm?id=1597966">paper</a> / <a href="https://youtu.be/xidrqSuyAa0">video</a></div>
							<div class="team-position"><span class="conference">VR学会論文誌</span><br/>
							<a href="https://doi.org/10.18974/tvrsj.15.3_397">paper</a></div>
						</div>
					</div>
                    <!-- end team member item -->

					</div>
			</div>
		</section>

		<section id="publications" class="mz-module">
			<div class="container light-bg">
        <div class="row">
					<div class="col-lg-12 text-center">
						<div class="section-title">
							<h2>Publications</h2>
						</div>
          </div>
				</div>
				<div class="row">
					<div class="text-left">
						<div class="mz-about-container">
							<ul class="category">
									<li>Journal Papers (Domestic / International)</li>
							</ul>
							<ol>
								<li>
										Ryota Gomi, Kazuki Takashima, Yuki Onishi, <span class="fujita">Kazuyuki Fujita</span>, Yoshifumi Kitamura. UbiSurface: A Robotic Touch Surface for Supporting Mid-air Planar Interactions in Room-Scale VR, Proc. of the ACM on Human-Computer Interaction, Vol. 7, Issue ISS, Article No. 443, pp. 376-397. [<a href="https://doi.org/10.1145/3626479">paper</a>]
								</li>
								<li>
										Kumpei Ogawa, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Yoshifumi Kitamura. Exploring Visual-Auditory Redirected Walking using Auditory Cues in Reality, IEEE Transactions on Visualization and Computer Graphics, to appear. [<a href="https://doi.org/10.1109/TVCG.2023.3309267">Early access paper</a>]
								</li>
								<li>
										<span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Yuichi Itoh, Yoshifumi Kitamura. Human-Workspace Interaction: prior research efforts and future challenges for supporting knowledge workers, Quality and User Experience, Vol. 8, Article no. 7, Aug. 2023. [<a href="https://doi.org/10.1007/s41233-023-00060-9">paper</a>]
								</li>
								<li>
										上堀まい，伊藤弘大，<span class="fujita">藤田和之</span>，伊藤雄一．口腔内への温度提示と食品の温度が食体験と味覚に与える影響．日本感性工学会論文誌，TJSKE-D-22-00070, 2023年3月. [<a href="https://doi.org/10.5057/jjske.TJSKE-D-22-00070">paper</a>]
								</li>
								<li>
										Takahiro Nagai, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Yoshifumi Kitamura. HandyGaze: A Gaze Tracking Technique for Room-Scale Environments using a Single Smartphone, Proc. of the ACM on Human-Computer Interaction (<b>ISS &#39;22</b>), Vol. 6, Issue ISS, Article No. 562, pp 143–160, Nov. 2022. [<a href="https://doi.org/10.1145/3567715">paper</a>]
								</li>
								<li>
										Taichi Tsuchida, <span class="fujita">Kazuyuki Fujita</span>, Kaori Ikematsu, Sayan Sarcar, Kazuki Takashima, Yoshifumi Kitamura. TetraForce: A Magnetic-Based Interface Enabling Pressure Force and Shear Force Input Applied to Front and Back of a Smartphone, Proc. of the ACM on Human-Computer Interaction (<b>ISS &#39;22</b>), Vol. 6, Issue ISS, Article No. 564, pp 185–206, Nov. 2022. [<a href="https://doi.org/10.1145/3567717">paper</a>]
								</li>
								<li>
										Yoshiki Kudo, Anthony Tang, <span class="fujita">Kazuyuki Fujita</span>, Isamu Endo, Kazuki Takashima, Yoshifumi Kitamura. Towards Balancing VR Immersion and Bystander Awareness, Proc. of the ACM on Human-Computer Interaction (<b>ISS &#39;21</b>), Vol. 5, Issue ISS, Article No. 484, pp 1–22, Nov. 2021 [<a href="https://doi.org/10.1145/3486950">paper</a>] <em>Best Paper Award</em>.
								</li>
								<li>
										鈴永紗也，<span class="fujita">藤田和之</span>，白井僚，伊藤雄一．CoiLED Display: 対象に巻きつけ可能なストライプ状フレキシブルディスプレイ．日本バーチャルリアリティ学会論文誌，Vol. 26, No. 4, pp. 230-240, 2021年12月. [<a href="https://doi.org/10.18974/tvrsj.26.4_230">paper</a>]
								</li>
								<li>
										上田将理，<span class="fujita">藤田和之</span>，伊藤雄一．PlanT：植物の成長制御を用いた積算情報可視化ディスプレイ．ヒューマンインタフェース学会論文誌，Vol. 23, No. 4, pp. 407-418, 2021年11月. [<a href="https://doi.org/10.11184/his.23.4_407">paper</a>]
								</li>
								<li>
										川崎祐太，伊藤雄一，<span class="fujita">藤田和之</span>，尾上孝雄．アクティブ音響センシングにおける環境温度変化にロバストな物体情報識別手法の検討．情報処理学会論文誌，Vol. 62, No. 10, pp. 1658-1668, 2021年10月. [<a href="http://doi.org/10.20729/00213193">paper</a>]
								</li>
								<li>
										Chiahuei Tseng, Miao Cheng, Hassan Matout, <span class="fujita">Kazuyuki Fujita</span>, Yoshifumi Kitamura, Satoshi Shioiri, I-Lien Ho, Asaf Bachrach. MA and Togetherness (Ittaikan) in the Narratives of Dancers and Spectators: Sharing an Uncertain Space. Jpn Psychol Res. Mar. 2021. [<a href="https://doi.org/10.1111/jpr.12330">paper</a>]
								</li>
								<li>
										<span class="fujita">藤田和之</span>，黄 梦婷，高嶋和毅，土田太一，真鍋宏幸，北村喜文．ShearSheet: 静電容量タッチパネルに装着可能な透明シートを用いた剪断入力インタフェース．情報処理学会論文誌，Vol. 62, No.2, pp. 641-653, 2021年2月. [<a href="http://id.nii.ac.jp/1001/00209337/">paper</a>]
								</li>
								<li>
										伊藤雄一，石原好貴，白井僚，<span class="fujita">藤田和之</span>，高嶋和毅，尾上孝雄．StickyTouch: 局所的粘着性を制御可能なタッチディスプレイ．日本バーチャルリアリティ学会論文誌，Vol. 25, No. 4, pp. 384-393, 2020年12月. [<a href="https://doi.org/10.18974/tvrsj.25.4_384">paper</a>] <em>第23回 論文賞</em>．
								</li>
								<li>
										<span class="fujita">藤田和之</span>，高嶋和毅，伊藤雄一，北村喜文．ペン入力環境におけるパン操作にズーム・チルトを連動させる地図ナビゲーション手法に関する調査．電子情報通信学会論文誌，Vol. J103-D, No. 11, pp. 817-828, 2020年11月． [<a href="https://search.ieice.org/bin/summary.php?id=j103-d_11_817">paper</a>]
								</li>
								<li>
										天間遼太郎，高嶋和毅，<span class="fujita">藤田和之</span>，末田航，北村喜文．空間連動する2つのカメラ視点を用いたドローン操縦インタフェースの拡張．情報処理学会論文誌，Vol. 61, No. 8, pp. 1319-1332, 2020年8月． [<a href="http://doi.org/10.20729/00206267">paper</a>]
								</li>
								<li>
										林大悟，<span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，Robert W. Lindeman，北村喜文．Redirected Jumping: VRにおける跳躍動作時の移動量操作手法．日本バーチャルリアリティ学会論文誌，Vol. 24, Nol. 4, pp. 341-350, 2019年12月．[<a href="https://doi.org/10.18974/tvrsj.24.4_341">paper</a>] <em>第22回 論文賞</em>．
								</li>
								<li>
										伊藤雄一，<span class="fujita"><span class="fujita">藤田和之</span></span>，城所宏行．パランガ：触覚フィードバックを持つ電子パラパラ漫画，日本バーチャルリアリティ学会論文誌，Vol. 19, No. 4, pp. 477-486, 2014年12月．[<a href="https://doi.org/10.18974/tvrsj.19.4_477">paper</a>]
								</li>
								<li>
										中島康祐，伊藤雄一，林勇介，池田和章，<span class="fujita"><span class="fujita">藤田和之</span></span>，尾上孝雄．Emoballoon：ソーシャルタッチインタラクションのための柔らかな風船型インタフェース，日本バーチャルリアリティ学会論文誌 Vol. 18, No. 3, pp. 255-265, 2013年9月．[<a href="https://doi.org/10.18974/tvrsj.18.3_255">paper</a>]
								</li>
								<li>
										<span class="fujita">Kazuyuki Fujita</span>, Yuichi Itoh, and Hiroyuki Kidokoro. “Paranga: An electronic flipbook that reproduces riffling interaction.” International Journal of Creative Interfaces and Computer Graphics (IJCICG), Vol. 4, No. 1, pp. 21-34, Jun.2013. [<a href="https://www.igi-global.com/article/paranga/84124">paper</a>]
								</li>
								<li>
										Yusuke Hayashi, Yuichi Itoh, Kazuki Takashima, <span class="fujita">Kazuyuki Fujita</span>, Kosuke Nakajima, and Takao Onoye. Cup-le: cup-shaped tool for subtly collecting Information during conversational experiment, In International Journal of Advanced Computer Science, Vol. 3, No. 1, pp. 44-50, Jan. 2013. [<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.428.9568">paper</a>]
								</li>
								<li>
										<span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，伊藤雄一，大崎博之，小野直亮，香川景一郎，津川翔，中島康祐，林勇介，岸野文郎．Ambient Suiteを用いたパーティ場面における部屋型会話支援システムの実装と評価，電子情報通信学会論文誌, Vol. J96-D, No. 1, 2013年1月．[<a href="https://ci.nii.ac.jp/naid/110009577695/">paper</a>]
								</li>
								<li>
										Ryusuke Endo, Yuichi Itoh, Kosuke Nakajima, <span class="fujita">Kazuyuki Fujita</span>, and Fumio Kishino. Digital Signage Supporting Collaborative Route Planning in Real Commercial Establishment. ICIC Express Letters, Vol. 6, No. 12, pp. 1-6, Dec. 2012.
								</li>
								<li>
										中島康祐，伊藤雄一，築谷喬之，<span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，岸野文郎．FuSA2 Touch Display: 大画面毛状マルチタッチディスプレイ，情報処理学会論文誌, Vol. 53, No. 3, pp. 1069-1081, 2012年3月．[<a href="http://id.nii.ac.jp/1001/00081283/">paper</a>]
								</li>
								<li>
										<span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，築谷喬之，朝日元生，伊藤雄一，北村喜文，岸野文郎．地図ナビゲーションにおけるパン操作とズーム/チルト連動を用いたビューポート制御手法の提案，電子情報通信学会論文誌, Vol. J93-D, No. 11, pp. 2454-2465, 2010年11月．[<a href="https://ci.nii.ac.jp/naid/110007880368/">paper</a>]
								</li>
								<li>
										<span class="fujita"><span class="fujita">藤田和之</span></span>，伊藤雄一，吉田愛，尾崎麻耶，菊川哲也，深澤遼，高嶋和毅，北村喜文，岸野文郎．アソブレラ：振動を記録・再生可能な傘型アンビエントインタフェース，日本バーチャルリアリティ学会論文誌, Vol. 15, No. 3, pp. 397-405, 2010年9月．[<a href="https://doi.org/10.18974/tvrsj.15.3_397">paper</a>]
								</li>
							</ol>
                            <ul>
                                <li class="category">Reviewed Conference Papers（Full / Short Paper）</li>
                            </ul>
                            <ol>
								<li>
										Maakito Inoue, Kazuki Takashima, <span class="fujita">Kazuyuki Fujita</span>, Yoshifumi Kitamura. BirdViewAR: Surroundings-aware Remote Drone Piloting Using an Augmented Third-person Perspective, Proc. of the 2023 CHI Conference on Human Factors in Computing Systems (<b>CHI &#39;23</b>), Article No. 31, pp. 1-19, May. 2023. [<a href="https://doi.org/10.1145/3544548.3580681">paper</a>]
								</li>
								<li>
										Yuki Onishi, Kazuki Takashima, Shoi Higashiyama, <span class="fujita">Kazuyuki Fujita</span>, Yoshifumi Kitamura. WaddleWalls: Room-scale Interactive Partitioning System using a Swarm of Robotic Partitions, Proc. of the 35th Annual ACM Symposium on User Interface Software and Technology (<b>UIST '22</b>), Article No.: 29, pp. 1–15, Oct. 2022. [<a href="https://doi.org/10.1145/3526113.3545615">paper</a>]
								</li>
								<li>
										Kumpei Ogawa, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Yoshifumi Kitamura. PseudoJumpOn: Jumping onto Steps in Virtual Reality, Proc. of 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (<b>IEEEVR '22</b>), pp. 635-643, Mar. 2022. [<a href="https://doi.org/10.1109/VR51125.2022.00084">paper</a>]
								</li>
								<li>
										Yukai Hoshikawa, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Morten Fjeld, Yoshifumi Kitamura. RedirectedDoors: Redirection While Opening Doors in Virtual Reality, Proc. of 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (<b>IEEEVR '22</b>), pp. 464-473, Mar. 2022. [<a href="https://doi.org/10.1109/VR51125.2022.00066">paper</a>] <em>Honorable Mention Award</em>.
								</li>
                                <li>
                                        Isamu Endo, Kazuki Takashima, Maakito Inoue, <span class="fujita">Kazuyuki Fujita</span>, Kiyoshi Kiyokawa, and Yoshifumi Kitamura. ModularHMD: A Reconfigurable Mobile Head-Mounted Display Enabling Ad-hoc Peripheral Interactions with the Real World, Proc. of the 34th Annual ACM Symposium on User Interface Software and Technology (<b>UIST &#39;21</b>), pp. 100-117, Oct. 2021. [<a href="https://doi.org/10.1145/3472749.3474738">paper</a>]
                                </li>
                                <li>
                                        <span class="fujita">Kazuyuki Fujita</span>, Aoi Suzuki, Kazuki Takashima, Kaori Ikematsu, Yoshifumi Kitamura. TiltChair: Manipulative Posture Guidance by Actively Inclining the Seat of an Office Chair, Proc. of the 2021 CHI Conference on Human Factors in Computing Systems (<b>CHI &#39;21</b>), Article No. 228, pp. 1-14, May. 2021. [<a href="https://doi.org/10.1145/3411764.3445151">paper</a>]
                                </li>
                                <li>
                                        Linfeng Chen, Kazuki Takashima, <span class="fujita">Kazuyuki Fujita</span>, Yoshifumi Kitamura. PinpointFly: An Egocentric Position-control Drone Interface using Mobile AR, Proc. of the 2021 CHI Conference on Human Factors in Computing Systems (<b>CHI &#39;21</b>), Article No. 150, pp. 1-13, May. 2021. [<a href="https://doi.org/10.1145/3411764.3445110">paper</a>]
                                </li>
                                <li>
                                        Yuki Onishi, Kazuki Takashima, <span class="fujita">Kazuyuki Fujita</span>, Yoshifumi Kitamura. BouncyScreen: Physical Enhancement of Pseudo-Force Feedback, Proc. of 2021 IEEE Conference on Virtual Reality and 3D User Interfaces (<b>IEEEVR '21</b>), pp. 363-372, Mar. 2021. [<a href="https://doi.org/10.1109/VR50410.2021.00059">paper</a>]
                                </li>
                                <li>
                                        Yan Yixian, Kazuki Takashima, Anthony Tang, Takayuki Tanno, <span class="fujita">Kazuyuki Fujita</span>, Yoshifumi Kitamura. ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World, Proc. of the 33rd Annual ACM Symposium on User Interface Software and Technology (<b>UIST &#39;20</b>), pp. 223-235, Oct. 2020. [<a href="https://doi.org/10.1145/3379337.3415859">paper</a>]
                                </li>
                                <li>
                                        Saya Suzunaga, Yuichi Itoh, Yuki Inoue, <span class="fujita">Kazuyuki Fujita</span>, Takao Onoye. TuVe : A Shape-changable Display using Fluids in a Tube, Proc. of the 2020 International Conference on Advanced Visual Interfaces (<b>AVI &#39;20</b>), pp. 1-9, Sep. 2020. [<a href="https://doi.org/10.1145/3399715.3399874">paper</a>]
                                </li>
                                <li>
                                        Yoshitaka Ishihara, Yuichi Itoh, Ryo Shirai, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Takao Onoye. StickyTouch: A Tactile Display with Changeable Adhesive Distribution, 2020 IEEE Haptics Symposium (<b>HAPTICS</b>), pp. 842-847, Mar. 2020. [<a href="https://doi.org/10.1109/HAPTICS45997.2020.ras.HAP20.4.b5d4a51b">paper</a>]
                                </li>
                                <li>
                                        Mengting Huang, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Taichi Tsuchida, Hiroyuki Manabe, Yoshifumi Kitamura. ShearSheet: Low-Cost Shear Force Input with Elastic Feedback for Augmenting Touch Interaction, Proc. of the 2019 ACM Interactive Surfaces and Spaces (<b>ISS &#39;19</b>), pp. 77-87, Nov. 2019. [<a href="https://doi.org/10.1145/3343055.3359717">paper</a>] <em>Best Demo Award</em>.
                                </li>
                                <li>
                                    	Ryotaro Temma, Kazuki Takashima, <span class="fujita">Kazuyuki Fujita</span>, Koh Sueda, Yoshifumi Kitamura. Third-Person Piloting: Increasing Situational Awareness using a Spatially Coupled Second Drone, Proc. of the 32nd Annual ACM Symposium on User Interface Software and Technology (<b>UIST &#39;19</b>), pp. 507-519, Oct. 2019. [<a href="https://doi.org/10.1145/3332165.3347953">paper</a>]
                                </li>
                                <li>
                                    	Pascal Chiu, Kazuki Takashima, <span class="fujita">Kazuyuki Fujita</span>, Yoshifumi Kitamura. Pursuit Sensing: Extending Hand Tracking Space in Mobile VR Applications, Proc. of Symposium on Spatial User Interaction (<b>SUI &#39;19</b>), 5 pages, Oct.2019. [<a href="https://doi.org/10.1145/3357251.3357578">paper</a>]
                                </li>
                                <li>
                                    	Daigo Hayashi, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Robert W. Lindeman, Yoshifumi Kitamura. Redirected Jumping: Imperceptibly Manipulating Jump Motions in Virtual Reality, Proc. of 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (<b>IEEEVR '19</b>) pp. 386-394, Mar. 2019. [<a href="https://ieeexplore.ieee.org/document/8797989">paper</a>]
                                </li>
                                <li>
                                    	Kosuke Nakajima, Yuichi Itoh, Yusuke Hayashi, Kazuaki Ikeda, <span class="fujita">Kazuyuki Fujita</span>, and Takao Onoye. Emoballoon: a Balloon-shaped Interface Recognizing Social Touch Interactions. Proc. of 10th International Conference on Advances in Computer Entertainment Technology (<b>ACE &#39;13</b>), pp. 182-197, Nov. 2013. [<a href="https://dl.acm.org/doi/10.5555/2770015.2770029">paper</a>]
                                </li>
                                <li>
                                    	<span class="fujita">Kazuyuki Fujita</span>, Hiroyuki Kidokoro, Yuichi Itoh. Paranga: An Interactive Flipbook, Proc. of the International Conference on Advances in Computer Entertainment Technology (<b>ACE &#39;12</b>), pp. 17-30, 2012. [<a href="https://link.springer.com/chapter/10.1007/978-3-642-34292-9_2">paper</a>]
                                </li>
                                <li>
                                    	Ryusuke Endo, Yuichi Itoh, Kosuke Nakajima, <span class="fujita">Kazuyuki Fujita</span>, Fumio Kishino. Planning-Capable Digital Signage System Using Multi-touch Display, Proc. of The 10th Asia Pacific Conference on Computer Human Interaction (<b>APCHI &#39;12</b>), Vol. 2, pp. 545-554, Aug. 2012.
                                </li>
                                <li>
                                    	<span class="fujita">Kazuyuki Fujita</span>, Yuichi Itoh, Hiroyuki Ohsaki, Naoaki Ono, Keiichiro Kagawa, Kazuki Takashima, Sho Tsugawa, Kosuke Nakajima, Yusuke Hayashi, Fumio Kishino. Ambient Suite: Enhancing Communication among Multiple Participants, Proc. of the International Conference on Advances in Computer Entertainment Technology (<b>ACE &#39;11</b>), pp. 25:1-25:8, Nov. 2011. [<a href="https://doi.org/10.1145/2071423.2071454">paper</a>]
                                </li>
                                <li>
                                    	Kosuke Nakajima, Yuichi Itoh, Takayuki Tsukitani, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Yoshifumi Kitamura, Fumio Kishino. FuSA2 Touch Display: Furry and Scalable Multi-touch Display, Proc. of ACM International Conference on Interactive Tabletops and Surfaces 2011 (<b>ITS &#39;11</b>), pp. 35-44, Nov. 2011. [<a href="https://ieeexplore.ieee.org/abstract/document/6180852">paper</a>]
                                </li>
                                <li>
                                    	<span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Takayuki Tsukitani, Yuichi Itoh, Yoshifumi Kitamura, Fumio Kishino. Anchored Navigation: Coupling Panning Operation with Zooming and Tilting Based on the Anchor Point on a Map, Proc. of the Graphics Interface 2010 (<b>GI &#39;10</b>), pp. 233-240, May. 2010. [<a href="https://dl.acm.org/doi/10.5555/1839214.1839255">paper</a>]
                                </li>
                                <li>
                                    	<span class="fujita">Kazuyuki Fujita</span>, Yuichi Itoh, Ai Yoshida, Maya Ozaki, Kikukawa Tetsuya, Ryo Fukazawa, Kazuki Takashima, Yoshifumi Kitamura, Fumio Kishino. Funbrella: Recording and Replaying Vibrations through an Umbrella Axis, Proc. of the International Conference on Advances in Computer Entertainment Technology (<b>ACE &#39;09</b>), pp. 66-71, Oct. 2009. [<a href="https://dl.acm.org/doi/10.1145/1690388.1690400">paper</a>]
                                </li>
                            </ol>
								<ul>
                                <li class="category">Reviewed Conference Papers（Poster / Demo / Workshop）</li>
                            </ul>
                            <ol>
								<li>
										Kumpei Ogawa, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Yoshifumi Kitamura. Demonstration of PseudoJumpOn: Repetitive Step-up Jump in Virtual Reality, SIGGRAPH Asia 2022 XR, Article No. 5, 2 pages, Dec. 2022. [<a href="https://doi.org/10.1145/3550472.3558412">paper</a>]
								</li>
								<li>
										Yukai Hoshikawa, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Morten Fjeld, Yoshifumi Kitamura. Demonstration of RedirectedDoors: Manipulating User’s Orientation while Opening Doors in Virtual Reality, SIGGRAPH Asia 2022 XR, Article No. 6, 2 pages, Dec. 2022. [<a href="https://doi.org/10.1145/3550472.3558405">paper</a>]
								</li>
								<li>
										Ryota Gomi, Kazuki Takashima, <span class="fujita">Kazuyuki Fujita</span>, Yoshifumi Kitamura. A Triangular Actuating Device Stand that Dynamically Adjusts Mobile Screen’s Position, Adjunct Proc. of the 35th Annual ACM Symposium on User Interface Software and Technology (<b>UIST &#39;22</b>), Article No.: 63, pp. 1-3, Oct. 2022. [<a href="https://doi.org/10.1145/3526114.3558637">paper</a>]
								</li>
								<li>
										Keito Uwaseki, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Yoshifumi Kitamura. ConfusionLens: Dynamic and Interactive Visualization for Performance Analysis of Multiclass Image Classifiers, Adjunct Proc. of the 35th Annual ACM Symposium on User Interface Software and Technology (<b>UIST &#39;22</b>), Article No.: 60, pp. 1-3, Oct. 2022. [<a href="https://doi.org/10.1145/3526114.3558631">paper</a>]
								</li>
								<li>
										Tatsuya Maeda, Keita Kuwayama, Kodai Ito, <span class="fujita">Kazuyuki Fujita</span>, Yuichi Itoh. FullPull: A Stretchable UI to Input Pulling Strength on Touch Surfaces, Adjunct Proc. of the 35th Annual ACM Symposium on User Interface Software and Technology (<b>UIST &#39;22</b>), Article No.: 52, pp. 1-3, Oct. 2022. [<a href="https://doi.org/10.1145/3526114.3558775">paper</a>]
								</li>
								<li>
										Taiyo Natomi, Yasuji Kitabatake, <span class="fujita">Kazuyuki Fujita</span>, Takao Onoye, and Yuichi Itoh, An Infant-Like Device that Reproduces Hugging Sensation with Multi-Channel Haptic Feedback, Proc. of the 27th ACM Symposium on Virtual Reality Software and Technology  (<b>VRST ’21</b>), Article No. 55, 3 pages, Nov. 2021. [<a href="https://doi.org/10.1145/3489849.3489927">paper</a>]
								</li>
                                <li>
                                    	Tatsuki Takano, Kazuki Takashima, <span class="fujita">Kazuyuki Fujita</span>, Hong Guang, Kaori Ikematsu, and Yoshifumi Kitamura. A Compact and Low-cost VR Tooth Drill Training System using Mobile HMD and Stylus Smartphone, Proc. of the 27th ACM Symposium on Virtual Reality Software and Technology (<b>VRST ’21</b>), Article No. 46, 3 pages, Dec. 2021. [<a href="https://doi.org/10.1145/3489849.3489933">paper</a>]
                                </li>
                                <li>
                                    	Isamu Endo, Kazuki Takashima, Maakito Inoue, <span class="fujita">Kazuyuki Fujita</span>, Kiyoshi Kiyokawa, and Yoshifumi Kitamura. A Reconfigurable Mobile Head-Mounted Display Supporting Real World Interactions, Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems (<b>CHI EA '21</b>), Article No. 418, pp. 1-7, May. 2021. [<a href="https://doi.org/10.1145/3411763.3451765">paper</a>]
                                </li>
                                <li>
                                    	Yuki Onishi, Kazuki Takashima, <span class="fujita">Kazuyuki Fujita</span>, and Yoshifumi Kitamura. Self-actuated Stretchable Partitions for Dynamically Creating Secure Workplaces, Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems (<b>CHI EA '21</b>), Article No. 294, pp. 1-6, May. 2021. [<a href="https://doi.org/10.1145/3411763.3451607">paper</a>]
                                </li>
                                <li>
                                    	Josh (Adi) Tedjasaputra, Briane Paul V. Samson, Masitah Ghazali, Eunice Sari, Sayan Sarcar, Dilrukshi Gamage, <span class="fujita">Kazuyuki Fujita</span>, Pranjal Jain, Amit Jena, Toni-Jan Keith Palma Monserrat, Nabila Sindi, Kaixing Zhao, Jordan Aiko Deja, Manvi Fotedar, Manjiri Joshi, Yang Li, Zhicong Lu, Akihiro Matsufuji, Shio Miyafuji, Korok Sengupta, Diksha Singh, Simran Singh, and Umar Taufiqulhakim. Asian CHI Symposium: HCI Research from Asia and on Asian Contexts and Cultures. Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems (<b>CHI EA '21</b>). Article No. 80, pp. 1–5, May. 2021. [<a href="https://doi.org/10.1145/3411763.3441341">paper</a>]
                                </li>
                                <li>
                                    	Saya Suzunaga, Yuichi Itoh, <span class="fujita">Kazuyuki Fujita</span>, Ryo Shirai, Takao Onoye. CoiLED Display: Make Everything Displayable, SIGGRAPH Asia 2020 Emerging Technologies. pp. 1-2, Dec. 2020. [<a href="https://doi.org/10.1145/3415255.3422889">paper</a>]
                                </li>
                                <li>
                                    	<span class="fujita">Kazuyuki Fujita</span>, Daigo Hayashi, Kotaro Hara, Kazuki Takashima, Yoshifumi Kitamura. Techniques to Visualize Occluded Graph Elements for 2.5D Map Editing. CHI 2020 Extended Abstract, pp. 1–9, Apr. 2020. [<a href="https://dl.acm.org/doi/abs/10.1145/3334480.3382987">paper</a>]
                                </li>
                                <li>
                                    	<span class="fujita">Kazuyuki Fujita</span>. Adaptive Spatial User Interfaces That Activate Us. The 26th International Display Workshops (<b>IDW '19</b>), AIS1/INP2-1 (Invited), pp. 13-15, Nov. 2019.
                                </li>
                                <li>
                                    	Tomas Havlik, Daigo Hayashi, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Robert W. Lindeman, Yoshifumi Kitamura. JumpinVR: Enhancing Jump Experience in a Limited Physical Space. SIGGRAPH Asia 2019 XR, Nov. 2019.
                                </li>
                                <li>
                                    	Pascal Chiu, Kazuki Takashima, <span class="fujita">Kazuyuki Fujita</span>, Yoshifumi Kitamura. FreeMo: Extending Hand Tracking Experiences Through Capture Volume and User Freedom. SIGGRAPH Asia 2019 XR, Nov. 2019.
                                </li>
                                <li>
                                    	Linfeng Chen, Akiyuki Ebi, Kazuki Takashima, <span class="fujita">Kazuyuki Fujita</span>, Yoshifumi Kitamura. PinpointFly: An Egocentric Position-pointing Drone Interfaceusing Mobile AR. SIGGRAPH Asia 2019 Emerging Technologies, Nov. 2019.
                                </li>
                                <li>
                                    	Yoshitaka Ishihara, Ryo Shirai, Yuichi Itoh, <span class="fujita">Kazuyuki Fujita</span>, Takao Onoye. StickyTouch: An Adhesion Changeable Surface. SIGGRAPH Asia 2019 Emerging Technologies, Nov. 2019. <em>Best Demo Voted by Committee</em>.
                                </li>
                                <li>
                                    	Xiyue Wang, Kaori Ikematsu, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Yoshifumi Kitamura. An Investigation of Electrode Design for Physical Touch Extensions on a Capacitive Touch Surface. Adjunct Proc. of the 32nd Annual ACM Symposium on User Interface Software and Technology (<b>UIST &#39;19</b>), pp. 66-68, Oct. 2019.
                                </li>
                                <li>
                                    	<span class="fujita">Kazuyuki Fujita</span>, Eunice Ratna Sari, Juho Kim, Adi B. Tedjasaputra, Ellen Yi-Luen Do, Zhengjie Liu, Uichin Lee, Toni-Jan Keith Palma Monserrat, Akihiro Matsufuji, Shio Miyafuji, Ryosuke Takada, Chat Wacharamanotham, Masitah Ghazali, Xiyue Wang, Thippaya Chintakovid, Kyoungwon Seo, Jinwoo Kim, Yoshifumi Kitamura. Asian CHI Symposium: Emerging HCI Research Collection. CHI Extended Abstracts 2019, No. Sym06, May. 2019.
                                </li>
                                <li>
                                    	Yoshitaka Ishihara, Shori Ueda, Yuichi Itoh, <span class="fujita">Kazuyuki Fujita</span>. PlanT: A Plant-based Ambient Display Visualizing Gradually Accumulated Information. Asian CHI Symposium: Emerging HCI Research Collection, No. 16, May. 2019. <em>Best Demo/Poster Award</em>.
                                </li>
                                <li>
                                    	Shotaro Ichikawa, Yuki Onishi, Daigo Hayashi, Akiyuki Ebi, Isamu Endo, Aoi Suzuki, Anri Niwano, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Yoshifumi Kitamura. Be Bait!: Hammock-based Interaction for Enjoyable Underwater Swimming in VR. Asian CHI Symposium: Emerging HCI Research Collection, No. 29, May. 2019.
                                </li>
                                <li>
                                    	Yoshifumi Kitamura, Kazuki Takashima, <span class="fujita">Kazuyuki Fujita</span>. Designing Dynamic Aware Interiors. Proc. of the 24th ACM Symposium on Virtual Reality Software and Technology (<b>VRST &#39;18</b>), pp. 77:1-77:2, Nov. 2018.
                                </li>
                                <li>
                                    	Xin Huang, Kazuki Takashima, <span class="fujita">Kazuyuki Fujita</span>, Yoshifumi Kitamura. Dynamic, Flexible and Multi-dimensional Visualization of Digital Photos and their Metadata. Proc. of the 2018 ACM International Conference on Interactive Surfaces and Spaces (<b>ISS &#39;18</b>), pp.405-408, Nov. 2018.
                                </li>
                                <li>
                                    	Yohei Kojima, Kazuma Aoyama, Yuichi Itoh, <span class="fujita">Kazuyuki Fujita</span>, Taku Fujimoto, and Kosuke Nakajima. Polka dot: the garden of water spirits. ACM SIGGRAPH 2013 Posters, Article No. 47, Jul. 2013.
                                </li>
                                <li>
                                    	Kosuke Nakajima, Yuichi Itoh, Yusuke Hayashi, Kazuaki Ikeda, <span class="fujita">Kazuyuki Fujita</span>, Takao Onoye. Emoballoon, Proc. of The 10th Asia Pacific Conference on Computer Human Interaction (<b>APCHI '12</b>), Vol. 2, pp. 681-682, Aug. 2012. <em>Best Poster / Best Demo Award</em>.
                                </li>
                                <li>
                                    	Sho Tsugawa, Yukiko Mogi, Yusuke Kikuchi, Fumio Kishino, <span class="fujita">Kazuyuki Fujita</span>, Yuichi Itoh, Hiroyuki Ohsaki. On estimating depressive tendencies of Twitter users utilizing their tweet data. Proc. of The 2nd International Workshop on Ambient Information Technologies (<b>AMBIT &#39;13</b>), pp.29-32, Mar. 2013.
                                </li>
                                <li>
                                    	Kosuke Nakajima, Yuichi Itoh, Yusuke Hayashi, Kazuaki Ikeda, <span class="fujita">Kazuyuki Fujita</span>, and Takao Onoye. Emoballoon: a Balloon-shaped Interface Recognizing Social Touch Interactions. Proc. of The 2nd International Workshop on Ambient Information Technologies (<b>AMBIT &#39;13</b>), pp.13-16, Mar. 2013.
                                </li>
                                <li>
                                    	Yuichi Fujii, Fumio Kishino, <span class="fujita">Kazuyuki Fujita</span>, Yuichi Itoh. U-brella: A portable umbrella-shaped device for vibrationizing information. Proc. of The 2nd International Workshop on Ambient Information Technologies (<b>AMBIT &#39;13</b>), pp.9-12, Mar. 2013.
                                </li>
                                <li>
                                    	<span class="fujita">Kazuyuki Fujita</span>, Yuichi Itoh, Kazuki Takashima, Kosuke Nakajima, Yusuke Hayashi, and Fumio Kishino. Ambient Party Room: A Room-shaped System Enhancing Communication for Parties or Gatherings. Proc. of The 2nd International Workshop on Ambient Information Technologies (<b>AMBIT &#39;13</b>), pp.1-4, Mar. 2013.
                                </li>
                                <li>
                                    	Kazuki Takashima, <span class="fujita">Kazuyuki Fujita</span>, Yuichi Itoh, Yoshifumi Kitamura. Elastic Scroll for Multi-focus Interactions, Adjunct Proc. of the 25th Annual ACM Symposium on User Interface Software and Technology (<b>UIST &#39;12</b>), pp. 19-20, Oct. 2012.
                                </li>
                                <li>
                                    	<span class="fujita">Kazuyuki Fujita</span>, Yuichi Itoh, Hiroyuki Ohsaki, Naoaki Ono, Keiichiro Kagawa, Kazuki Takashima, Sho Tsugawa, Kosuke Nakajima, Yusuke Hayashi, Fumio Kishino. Ambient Suite: Room-shaped Information Environment for Interpersonal Communication, Proc. of The 1st International Workshop on Ambient Information Technologies (<b>AMBIT &#39;12</b>), pp. 18-21, Mar. 2012.
                                </li>
                                <li>
                                    	Kosuke Nakajima, Yuichi Itoh, Takayuki Tsukitani, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Yoshifumi Kitamura, Fumio Kishino. FuSA2 Touch Display: Furry and Scalable Multi-touch Display, Proc. of The 1st International Workshop on Ambient Information Technologies (<b>AMBIT &#39;12</b>), pp. 35-36, Mar. 2012.
                                </li>
                                <li>
                                    	Yusuke Hayashi, Yuichi Itoh, Kazuki Takashima, <span class="fujita">Kazuyuki Fujita</span>, Kosuke Nakajima, Ikuo Daibo, Takao Onoye. Cup-le: A Cup-Shaped Device for Conversational Experiment, Proc. of The 1st International Workshop on Ambient Information Technologies (<b>AMBIT &#39;12</b>), pp. 36-37, Mar. 2012.
                                </li>
                                <li>
                                    	Hiroyuki Kidokoro, <span class="fujita">Kazuyuki Fujita</span>, Masanori Owaki, Khoa Doba, Christopher Chung, Yuichi Itoh. Paranga: A Book-shaped Device with Tactile Feedback, The 4th ACM SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia (SIGGRAPH Asia 2011) Emerging Technologies, Dec. 2011.
                                </li>
                                <li>
                                    	Ai Yoshida, Yuichi Itoh, <span class="fujita">Kazuyuki Fujita</span>, Maya Ozaki, Kikukawa Tetsuya, Ryo Fukazawa, Yoshifumi Kitamura, Fumio Kishino. Funbrella: Making Rain Fun, ACM SIGGRAPH 2009 Emerging Technologies, No. 10, Aug. 2009.
                                </li>
                            </ol>
                            <ul>
                                <li class="category">Reviewed Domestic Conference Papers</li>
                            </ul>
							<ol>
								<li>
										前田竜矢，<span class="fujita">藤田和之</span>，伊藤弘大，桑山佳汰，藤田和之，伊藤雄一．サーフェスにおける引っ張り入力を実現するインタフェースの検討．インタラクション2023論文集，pp. 88-96, 2023年3月．
								</li>
								<li>
										土田太一，<span class="fujita">藤田和之</span>，池松香，Sayan Sarcar，高嶋和毅，北村喜文．TetraForce: スマートフォンの表裏両面に対する垂直・剪断方向の力を入力可能な磁気式インタフェース．第30回インタラクティブシステムとソフトウェアに関するワークショップ (WISS '22) 論文集，2022年12月．
								</li>
                                <li>
                                    	鄢一先，高嶋和毅，アンソニー タン，<span class="fujita">藤田和之</span>，北村喜文．複数の自走壁型プロップを用いたルームスケールVRの空間インフラの遭遇型触覚提示．インタラクション2020論文集，pp. 123-132, 2020年3月．
                                </li>
                                <li>
                                    	上田将理，伊藤雄一，<span class="fujita"><span class="fujita">藤田和之</span></span>，尾上孝雄．PlanT：植物を用いた積算情報可視化ディスプレイ．インタラクション2020論文集，pp. 21-30, 2020年3月．
                                </li>
                                <li>
                                    	天間遼太郎，高嶋和毅，末田航，<span class="fujita"><span class="fujita">藤田和之</span></span>，北村喜文．空間連動する2つのカメラ視点を用いたドローン操縦インタフェースの拡張．インタラクション2019論文集，pp. 102-111, 2019年3月．
                                </li>
                                <li>
                                    	工藤義礎，アンソニー タン，<span class="fujita"><span class="fujita">藤田和之</span></span>，遠藤勇，高嶋和毅，ソール グリーンバーグ，北村喜文．近接学に基づくHMD利用者・非利用者の間の段階的なアウェアネスの向上．インタラクション2019論文集，pp. 48-57, 2019年3月．
                                </li>
                                <li>
                                    	中島康祐，伊藤雄一，林勇介，池田和章，<span class="fujita"><span class="fujita">藤田和之</span></span>，尾上孝雄. Emoballoon: ソーシャルタッチインタラクションのための風船型インタフェース. インタラクション2013論文集, pp. 95-102, 2013年3月．
                                </li>
                                <li>
                                    	大脇正憲，<span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，築谷喬之，伊藤雄一，北村喜文，岸野文郎．撓みのメタファを用いたビューポート制御インタフェース，インタラクション2011論文集, pp. 115-122, 2011年3月．
                                </li>
                                <li>
                                    	<span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，築谷喬之，朝日元生，北村喜文，岸野文郎．複数のカメラ操作を連動させる地図ナビゲーション手法の提案，インタラクション2009 論文集, pp. 97-104, 2009年3月．
                                </li>
                            </ol>
                            <ul>
                                <li class="category">Non-reviewed Domestic Conference Papers</li>
                            </ul>
                            <ol>
								<li>
										小川郡平，<span class="fujita">藤田和之</span>，高嶋和毅，北村喜文．ハンドリダイレクションを用いた広範囲のキャンバスへのVRドローイング体験．第31回インタラクティブシステムとソフトウェアに関するワークショップ (WISS '23) 論文集，2023年12月．<em>対話発表賞（プログラム委員）．</em>
								</li>
								<li>
										橘優希，<span class="fujita">藤田和之</span>，高嶋和毅，北村喜文．スマートフォンのみによる視線トラッキング技術を用いた展示案内アプリケーション．インタラクション2023論文集，pp. 981-986, 2023年3月．<em>インタラクティブ発表賞（一般投票）．</em>
								</li>
								<li>
										田中雄大，<span class="fujita">藤田和之</span>，高嶋和毅，北村喜文．デスクワーク中の頭部姿勢に応じた空間音響フィードバック提示手法．インタラクション2023論文集，pp. 820-824, 2023年3月．
								</li>
								<li>
										中川久倫，伊藤弘大，<span class="fujita">藤田和之</span>，岸楓馬，福島力也，伊藤雄一．エクスカキバー：ビジュアル・サウンドエフェクトを用いた筆記支援．インタラクション2022論文集，pp. 641-644, 2022年3月．<em>インタラクティブ発表賞（PC推薦・一般投票）．</em>
								</li>
								<li>
										井熊勇介，伊藤弘大，<span class="fujita">藤田和之</span>，角谷星哉，名富太陽，物永斉，伊藤雄一．床型デバイスを用いた歩容取得による人物の推定．インタラクション2022論文集，pp. 684-686, 2022年3月．
								</li>
								<li>
										三津谷海度，伊藤弘大，<span class="fujita">藤田和之</span>，岸楓馬，福島力也，伊藤雄一．SenseDesk：重心重量によりユーザの状態推定が可能なデスクの開発．インタラクション2022論文集，pp. 687-689, 2022年3月．
								</li>
								<li>
										加藤麻奈，伊藤弘大，<span class="fujita">藤田和之</span>，伊藤雄一，植物のメタファを用いたテレワーカーの感性的状態の表現方法の調査，研究報告ヒューマンコンピュータインタラクション（HCI），Vol. 2022-HCI-196, No. 17, pp. 1-8, 2022年1月．<em>学生奨励賞．</em>
								</li>
                                <li>
                                        星川結海，<span class="fujita">藤田和之</span>，高嶋和毅，北村喜文，VRにおける開扉時の視触覚を用いたリダイレクション手法の提案，日本バーチャルリアリティ学会第26回大会論文集, pp. 2B3-7, 2021年9月．
                                </li>
                                <li>
                                        小川郡平，<span class="fujita">藤田和之</span>，高嶋和毅，北村喜文，VRにおける段差への擬似的な跳び乗り感覚提示手法の提案，日本バーチャルリアリティ学会第26回大会論文集, pp. 2D3-3, 2021年9月．
                                </li>
                                <li>
                                        名富太陽，北畠康司，<span class="fujita">藤田和之</span>，尾上孝雄，伊藤雄一，乳児型デバイスを用いた乳児の抱擁感覚再現手法，日本バーチャルリアリティ学会第26回大会論文集, pp. 2B3-1, 2021年9月．
                                </li>
                                <li>
                                        永井崇大，<span class="fujita">藤田和之</span>，高嶋和毅，北村喜文，スマートフォンのみを用いた周囲環境への視線入力インタフェースの検討，ヒューマンインタフェース学会研究報告集, 2021年6月．
                                </li>
                                <li>
                                    角谷星哉，<span class="fujita">藤田和之</span>，尾上孝雄，伊藤雄一，ベビーベッド型デバイスを用いた乳児の啼泣認識手法の検討，ヒューマンインタフェース学会研究報告集, 2021年6月．
                                </li>
								<li>
									遠藤勇，高嶋和毅，井上理哲人，清川清，<span class="fujita">藤田和之</span>，北村 喜文．周辺とのアドホックなインタラクションを実現する再構成可能なVR HMD．インタラクション2021論文集，pp. 271-273, 2021年3月．<em>インタラクティブ発表賞（PC推薦）．</em>
								</li>
								<li>
									鈴木蒼生，<span class="fujita">藤田和之</span>，高嶋和毅，池松香，北村喜文．TiltChair:座面の前傾により姿勢誘導するオフィスチェア．インタラクション2021論文集，pp. 134-139, 2021年3月．
								</li>
								<li>
                                    川崎祐太，伊藤雄一，<span class="fujita">藤田和之</span>，尾上孝雄，アクティブ音響センシングを用いた物体情報識別における環境温度変化に関する一検討，ヒューマンインタフェース学会研究報告集, 2020年10月．
                                </li>
                                <li>
                                    鈴永紗也，伊藤雄一，<span class="fujita">藤田和之</span>，白井僚，尾上孝雄，ストライプ状LED群を用いたフレキシブルディスプレイ，ヒューマンインタフェース学会研究報告集, 2020年10月．
                                </li>
                                <li>
                                    西村賢人，伊藤雄一，藤原健，<span class="fujita">藤田和之</span>，松井裕子，彦野賢，尾上孝雄．SenseChairによる意見発散課題におけるコミュニケーションとうなずきの関係性に関する検討．信学技報, Vol. 120, No. 136, HCS2020-32, pp. 65-70, 2020年8月．
                                </li>
                                <li>
                                    泉健太, 鈴木蒼生, 市川将太郎, 高嶋和毅, <span class="fujita">藤田和之</span>, 北村喜文．ラバーハンド錯覚を利用した身体への映像と触覚の同時提示に関する検討．ヒューマンインタフェース学会研究報告集, Vol. 22, pp. 37-42，2020年5月．
                                </li>
                                <li>
									土田太一，<span class="fujita">藤田和之</span>，中原和洋，山田茂雄，高嶋和毅，北村喜文．バーチャルな影による奥行き錯覚を用いた付箋紙の移動操作と類似度可視化インタフェース．インタラクション2020論文集，pp. 848-851, 2020年3月．<em>プレミアム発表．</em>
								</li>
								<li>
									中原和洋，<span class="fujita">藤田和之</span>，土田太一，高嶋和毅，北村喜文．机上へのプロジェクションによる紙面テキストハイライト手法．インタラクション2020論文集，pp. 909-911, 2020年3月．<em>プレミアム発表．</em>
								</li>
								<li>
									林大悟，<span class="fujita">藤田和之</span>，原航太郎，高嶋和毅，北村喜文．地図グラフデータ内のトポロジー誤り発見・修正のためのインタラクション手法．インタラクション2020論文集，pp. 639-641, 2020年3月．
								</li>
								<li>
	                              	遠藤勇，工藤義礎，高嶋和毅，<span class="fujita">藤田和之</span>，北村喜文．周辺環境への気づきやインタラクションが可能なHMDに関する検討．日本バーチャルリアリティ学会第24回大会論文集，pp. 6C-01，2019年9月．
								</li>
								<li>
                                    大西悠貴，高嶋和毅，<span class="fujita">藤田和之</span>，北村喜文．平面ディスプレイの移動による擬似力覚の生成に関する研究．日本バーチャルリアリティ学会第24回大会論文集，pp. 1A-07，2019年9月．
                                </li>
                                <li>
                                    海老晃行，陳林峰，高嶋和毅，<span class="fujita">藤田和之</span>，北村喜文．ドローン操縦時の空間把握のための操縦状況に連携した副ドローンカメラ自動配置手法．ヒューマンインタフェースシンポジウム2019論文集， 2019年9月．
                                </li>
                                <li>
                                    <span class="fujita">藤田和之</span>．CHIワークショップ開催報告．情報処理学会研究報告，Vol. 2019-HCI-184, No. 11, 2019年7月.
                                </li>
                                <li>
                                    鈴木蒼生，<span class="fujita">藤田和之</span>，高嶋和毅，北村喜文．座面の座り心地を制御可能な椅子に関する一検討．ヒューマンインタフェース学会研究報告集，Vol. 21, No. 4, pp. 37-40. 2019年6月．
                                </li>
                                <li>
                                    鈴永紗也，伊藤雄一，<span class="fujita"><span class="fujita">藤田和之</span></span>，尾上孝雄．ガラス管を用いたボリューメトリックディスプレイのための気泡位置制御．ヒューマンインタフェース学会研究報告集，Vol. 21, No. 4, pp. 31-36. 2019年6月．
                                </li>
                                <li>
                                    井上理哲人，天間遼太郎，<span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，北村喜文．ドローンとARを用いて交差点の死角を可視化するユーザインタフェース．電子情報通信学会総合大会論文集，pp. ISS-SP-078. 2019年3月．
                                </li>
                                <li>
									Mengting Huang，<span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，真鍋宏幸，北村喜文．タッチスクリーン上に重ねた透明シートを利用した位置と速度制御の併用が可能なユーザインタフェース．インタラクション2019論文集，pp. 276-278, 2019年3月．<em>インタラクティブ発表賞（PC推薦）．</em>
								</li>
								<li>
                                    <span class="fujita"><span class="fujita">藤田和之</span></span>，西川政行，小笠原豊，白鳥毅，大橋一広，会議支援のための情報表出空間の構築，第31回人工知能学会全国大会，2P1-4，2017年5月．
                                </li>
                                <li>
                                    津川翔，茂木佑希子，菊地佑介，岸野文郎，<span class="fujita"><span class="fujita">藤田和之</span></span>，伊藤雄一，大崎博之，Twitter におけるユーザの活動履歴を利用したうつ傾向の推定に関する一検討，第 33 回インターネット技術第 163 委員会研究会, 2013年5月.
                                </li>
                                <li>
                                    津川翔，茂木佑希子，菊地佑介，岸野文郎，<span class="fujita"><span class="fujita">藤田和之</span></span>，伊藤雄一，大崎博之，Twitter 解析によるうつ傾向推定に関する一検討，電子情報通信学会総合大会講演論文集 (A-14-6), p.187, 2013年3月．
                                </li>
                                <li>
                                    脇田昌紀，岸野文郎，<span class="fujita"><span class="fujita">藤田和之</span></span>，伊藤雄一．SenseChairを用いた同調傾向の計測，電子情報通信学会総合大会講演論文集 (A-14-5), p.186, 2013年3月．
                                </li>
                                <li>
                                    津川翔，茂木佑希子，菊地佑介，岸野文郎，<span class="fujita"><span class="fujita">藤田和之</span></span>，伊藤雄一，大崎博之，大規模ツイートデータを利用したうつ傾向の推定に関する検討，電子情報通信学会技術研究報告 (HCS2012-89), pp.61-66, 2013年2月．
                                </li>
                                <li>
                                    <span class="fujita"><span class="fujita">藤田和之</span></span>，伊藤雄一，高嶋和毅，中島康祐，林勇介，岸野文郎．Ambient Party Room: パーティ場面における部屋型会話支援システムの構築，第91回ヒューマンインタフェース学会研究報告集，Vol. 14, No. 8, pp. 7-10, 2012年9月．
                                </li>
                                <li>
                                    <span class="fujita"><span class="fujita">藤田和之</span></span>，城所宏行，伊藤雄一．アナログパラパラデジタルマンガ，日本バーチャルリアリティ学会第17回大会 オーガナイズドセッション, 2012年9月．
                                </li>
                                <li>
                                    藤井佑一，岸野文郎，<span class="fujita"><span class="fujita">藤田和之</span></span>，中島康祐，伊藤雄一，菊地日出男．U-brella: 降り注ぐ情報を可振化するポータブル傘型インタフェース，日本バーチャルリアリティ学会第17回大会論文集，pp. 652-655, 2012年9月．
                                </li>
                                <li>
                                    高嶋和毅，<span class="fujita"><span class="fujita">藤田和之</span></span>，横山ひとみ，伊藤雄一，北村喜文．6人会話における非言語情報と場の活性度に関する検討，電子情報通信学会技術研究報告，Vol. 112, No. 176 pp. 49-54, 2012年8月．
                                </li>
                                <li>
                                    遠藤隆介，伊藤雄一，中島康祐，<span class="fujita"><span class="fujita">藤田和之</span></span>，岸野文郎．マルチタッチディスプレイを用いたプランニングができるデジタルサイネージシステムの提案，ヒューマンインタフェース学会研究報告集, Vol. 14, No. 8, 2012年6月．
                                </li>
                                <li>
                                    児島陽平，伊藤雄一，<span class="fujita"><span class="fujita">藤田和之</span></span>，中島康祐，尾上孝雄．空間内の複数人員配置のための指示位置提示手法に関する検討，ヒューマンインタフェース学会研究報告集, Vol. 14, No. 8, 2012年6月．
                                </li>
                                <li>
                                    竹中拓也，岸野文郎，<span class="fujita"><span class="fujita">藤田和之</span></span>，中島康祐，伊藤雄一．二者間の着座状態と会話の活性度の関係に関する検討，電子情報通信学会総合大会論文集, pp. A-14-2, 2012年3月．
                                </li>
                                <li>
                                    新宅彩加，岸野文郎，石原のぞみ，<span class="fujita"><span class="fujita">藤田和之</span></span>，伊藤雄一．書籍固有の情報を用いた書籍の明るさ判定，電子情報通信学会総合大会論文集, pp. A-16-19, 2012年3月．
                                </li>
                                <li>
                                    林勇介，伊藤雄一，中島康祐，<span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，大坊郁夫，尾上孝雄．カップ型デバイス Cup-le を用いた会話実験支援手法，ヒューマンインタフェースシンポジウム2011論文集, pp. 405-408, 2011年9月．
                                </li>
                                <li>
                                    <span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，伊藤雄一，大崎博之，小野直亮，香川景一郎，津川翔，中島康祐，林勇介，岸野文郎．Ambient Suite: 部屋型情報空間を用いた対人コミュニケーション支援，ヒューマンインタフェースシンポジウム2011論文集, pp. 395-400, 2011年9月．
                                </li>
                                <li>
                                    大脇正憲，<span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，伊藤雄一，北村喜文．マルチタッチ入力環境における撓みスクロール・ズーム手法，ヒューマンインタフェースシンポジウム2011論文集, pp. 429-434, 2011年9月．<em>学術奨励賞．</em>
                                </li>
                                <li>
                                    藤井佑一，岸野文郎，<span class="fujita"><span class="fujita">藤田和之</span></span>，伊藤雄一．振動ディスプレイを用いた情報可振化インタフェースの一検討，日本バーチャルリアリティ学会第16回大会論文集, pp. 14C-1, 2011年9月．
                                </li>
                                <li>
                                    前田奈穂，大坊郁夫，<span class="fujita"><span class="fujita">藤田和之</span></span>．関係開始スキルがパーティ場面におけるコミュニケーション行動に及ぼす影響，電子情報通信学会技術研究報告, Vol. 111, No. 190, pp. 5-10, 2011年8月．
                                </li>
                                <li>
                                    藤井佑一，藤川翔平，岸野文郎，<span class="fujita"><span class="fujita">藤田和之</span></span>，伊藤雄一．情報可振化インタフェース実現のための一検討，電子情報通信学会総合大会論文集, pp. A-15-20, 2011年3月．
                                </li>
                                <li>
									城所宏行，<span class="fujita"><span class="fujita">藤田和之</span></span>，大脇正憲，Khoa Doba，Christopher Chung，伊藤雄一．パランガ：ページをめくる触感を再現する本型デバイス，インタラクション2011 論文集, pp. 609-612, 2011年3月．
								</li>
								<li>
									中島康祐，伊藤雄一，築谷喬之，<span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，岸野文郎．FuSA2 Touch Display: 大画面毛状マルチタッチディスプレイ，インタラクション2011論文集, pp. 547-550, 2011年3月．
								</li>
								<li>
                                    <span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，築谷喬之，伊藤雄一，北村喜文，岸野文郎．地図ナビゲーションにおけるバーチャルカメラのパン・ズーム・チルトの連動に関する検討，日本バーチャルリアリティ学会第15回大会論文集, pp. 3B1-2, 2010年9月．
                                </li>
                                <li>
                                    吉田愛，伊藤雄一，尾崎麻耶，菊川哲也，深澤遼，<span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，北村喜文，岸野文郎．アソブレラ：雨の振動を記録・再生する傘型デバイス，インタラクティブ東京2009シンポジウム，2009年10月．
                                </li>
                                <li>
                                    <span class="fujita"><span class="fujita">藤田和之</span></span>，伊藤雄一，吉田愛，尾崎麻耶，菊川哲也，深澤遼，高嶋和毅，北村喜文，岸野文郎．アソブレラ：雨と遊ぶ，エンタテインメントコンピューティング2009 (EC2009) 論文集, pp. 73-74, 2009年9月．
                                </li>
                                <li>
                                    <span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，築谷喬之，北村喜文，岸野文郎．パン操作にズームとチルトを連動させる地図ナビゲーション，画像の認識・理解シンポジウム（MIRU2009）論文集, pp. 1875-1876, 2009年7月．
                                </li>
                                <li>
                                    吉田愛，伊藤雄一，尾崎麻耶，菊川哲也，深澤遼，<span class="fujita"><span class="fujita">藤田和之</span></span>，北村喜文，岸野文郎．アソブレラ：傘軸の振動を記録・再生するシステムの検討，電子情報通信学会技術研究報告, Vol. 109, No. 75, pp. 65-68, 2009年6月．
                                </li>
                            </ol>
                            <ul>
                                <li class="category">Patents</li>
                            </ul>
                            <ol>
																<li>椅子、前傾角度変更装置、及び、前傾角度変更方法，特願2021-036737．</li>
															  <li>コミュニケーション支援システム，特願2016-093239，2016年5月6日，特開2017-201479，2017年11月9日．</li>
                                <li>表示装置およびプログラム，特願2011-051234，2011年3月9日．</li>
                                <li>他 出願 6件</li>
                            </ol>
                            <ul>
                                <li class="category">Awards</li>
                            </ul>
                            <ol>
								<li>
										橘優希，<span class="fujita">藤田和之</span>，高嶋和毅，北村喜文．スマートフォンのみによる視線トラッキング技術を用いた展示案内アプリケーション．インタラクション2023論文集，pp. 981-986, 2023年3月．<em>インタラクティブ発表賞（一般投票）．</em>
								</li>
								<li>
										Yukai Hoshikawa, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Morten Fjeld, Yoshifumi Kitamura. RedirectedDoors: Redirection While Opening Doors in Virtual Reality, Proc. of 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (<b>IEEEVR '22</b>), pp. 464-473, Mar. 2022. <em>Honorable Mention Award</em>.
								</li>
								<li>
										中川久倫，伊藤弘大，<span class="fujita">藤田和之</span>，岸楓馬，福島力也，伊藤雄一．エクスカキバー：ビジュアル・サウンドエフェクトを用いた筆記支援．インタラクション2022論文集，pp. 641-644, 2022年3月．<em>インタラクティブ発表賞（PC推薦・一般投票）．</em>
								</li>
								<li>
										加藤麻奈，伊藤弘大，<span class="fujita">藤田和之</span>，伊藤雄一，植物のメタファを用いたテレワーカーの感性的状態の表現方法の調査，研究報告ヒューマンコンピュータインタラクション（HCI），Vol. 2022-HCI-196, No. 17, pp. 1-8, 2022年1月．<em>学生奨励賞．</em>
								</li>
								<li>
										Yoshiki Kudo, Anthony Tang, <span class="fujita">Kazuyuki Fujita</span>, Isamu Endo, Kazuki Takashima, Yoshifumi Kitamura. Towards Balancing VR Immersion and Bystander Awareness, Proc. of the ACM on Human-Computer Interaction (<b>ISS &#39;21</b>), Vol. 5, Issue ISS, Article No. 484, pp 1–22, Nov. 2021 [<a href="https://doi.org/10.1145/3486950">paper</a>] <em>Best Paper Award</em>.
								</li>
								<li>
										伊藤雄一，石原好貴，白井僚，<span class="fujita">藤田和之</span>，高嶋和毅，尾上孝雄．StickyTouch: 局所的粘着性を制御可能なタッチディスプレイ．日本バーチャルリアリティ学会論文誌，Vol. 25, No. 4, pp. 384-393, 2020年12月. [<a href="https://doi.org/10.18974/tvrsj.25.4_384">paper</a>] <em>第23回 論文賞</em>．
								</li>
								<li>
										遠藤勇，高嶋和毅，井上理哲人，清川清，<span class="fujita">藤田和之</span>，北村 喜文．周辺とのアドホックなインタラクションを実現する再構成可能なVR HMD．インタラクション2021論文集，pp. 271-273, 2021年3月．<em>インタラクティブ発表賞（PC推薦）．</em>
								</li>
								<li>
										林大悟，<span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，Robert W. Lindeman，北村喜文．Redirected Jumping: VRにおける跳躍動作時の移動量操作手法．日本バーチャルリアリティ学会論文誌，Vol. 24, Nol. 4, pp. 341-350, 2019年12月．[<a href="https://doi.org/10.18974/tvrsj.24.4_341">paper</a>] <em>第22回 論文賞</em>．
								</li>
								<li>
										Yoshitaka Ishihara, Ryo Shirai, Yuichi Itoh, <span class="fujita">Kazuyuki Fujita</span>, Takao Onoye. StickyTouch: An Adhesion Changeable Surface. SIGGRAPH Asia 2019 Emerging Technologies, Nov. 2019. <em>Best Demo Voted by Committee (1 out of 27 presentations)</em>.
								</li>
								<li>
										Mengting Huang, <span class="fujita">Kazuyuki Fujita</span>, Kazuki Takashima, Taichi Tsuchida, Hiroyuki Manabe, Yoshifumi Kitamura. ShearSheet: Low-Cost Shear Force Input with Elastic Feedback for Augmenting Touch Interaction, Proc. of ISS &#39;19, pp.77-87, Nov. 2019. <em>Best Demo Award (1 out of 14 demos)</em>.
								</li>
                                <li>
                                    Yoshitaka Ishihara, Shori Ueda, Yuichi Itoh, <span class="fujita">Kazuyuki Fujita</span>. PlanT: A Plant-based Ambient Display Visualizing Gradually Accumulated Information. Asian CHI Symposium: Emerging HCI Research Collection, May. 2019. <em>Best Demo/Poster Awards (5 out of 52 presentations)</em>.
                                </li>
                                <li>
                                    Mengting Huang，<span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，真鍋宏幸，北村喜文．タッチスクリーン上に重ねた透明シートを利用した位置と速度制御の併用が可能なユーザインタフェース．インタラクション2019論文集，pp. 276-278, 2019年3月．<em>インタラクティブ発表賞（PC推薦，214件中7件）．</em>
                                </li>
                                <li>
                                    Kosuke Nakajima, Yuichi Itoh, Yusuke Hayashi, Kazuaki Ikeda, <span class="fujita">Kazuyuki Fujita</span>, Takao Onoye. Emoballoon. The 10th Asia Pacific Conference on Computer Human Interaction (APCHI2012), Aug. 2012. <em>Best Poster / Demonstration Award</em>.
                                </li>
                                <li>
                                    大脇正憲，<span class="fujita"><span class="fujita">藤田和之</span></span>，高嶋和毅，伊藤雄一，北村喜文．マルチタッチ入力環境における撓みスクロール・ズーム手法，ヒューマンインタフェースシンポジウム2011論文集, pp. 429-434, 2011年9月．第12回 ヒューマンインタフェース学会 <em>学術奨励賞</em> (2011年度)，
                                </li>
                                <li>
                                    第18回 国際学生対抗バーチャルリアリティコンテスト (IVRC2010) <em>明和電機社長賞</em>，作品名：パランガ，2010年8月．
                                </li>
                                <li>
                                    第16回 国際学生対抗バーチャルリアリティコンテスト (IVRC2008) 東京予選大会 ハンズオン部門 <em>優勝</em>．作品名：アソブレラ，2008年9月．
                                </li>
                                <li>
                                    第16回 国際学生対抗バーチャルリアリティコンテスト (IVRC2008) 岐阜本戦大会 <em>審査員特別賞，各務原市民賞</em>．作品名：アソブレラ，2008年11月．
                                </li>
                            </ol>
                            <ul>
                                <li class="category">Invited Talks</li>
                            </ul>
                            <ol>
                                <li>
                                    <span class="fujita">Kazuyuki Fujita</span>. Adaptive Spatial User Interfaces That Activate Us. The 26th International Display Workshops (IDW '19), AIS1/INP2-1 (Invited), pp. 13-15, Nov. 2019.
                                </li>
                                <li>
                                    <span class="fujita">藤田和之</span>．CHIワークショップ開催報告．情報処理学会研究報告，Vol. 2019-HCI-184, No. 11, 2019年7月.
                                </li>
                                <li>
                                    働き方変革と未来のワークプレイス，東北大学，第4回 共同プロジェクト研究会「人と空間と情報技術」，2018年3月．
                                </li>
                                <li>
                                    山田茂雄，<span class="fujita"><span class="fujita">藤田和之</span></span>．アイデアが生まれる空間に住まう「もうひとりの参加者」，東京大学，日本ロボット学会セミナー 第107回 インタラクションにより人や環境に適応するロボット・ＡＩの行動戦略，2017年8月．
                                </li>
                                <li>
                                    会議支援のための情報表出空間，東北大学，第3回 共同プロジェクト研究会「人と空間と情報技術」，2017年3月．
                                </li>
                                <li>
                                    Office Vision 2020 ～近未来の社会情勢予測から逆引きするオフィス像～，東北大学，第2回 共同プロジェクト研究会「人と空間と情報技術」，2016年2月．
                                </li>
                                <li>
                                    協創空間 ～トランスボーダーなクリエイティブ・プラットフォームへの仕掛け～，スルガ銀行d-laboミッドタウン セミナー，2015年8月．
                                </li>
                                <li>
                                    イトーキとICT，東北大学，第1回 共同プロジェクト研究会「人と空間と情報技術」，2015年3月．
                                </li>
                                <li>
                                    <span class="fujita"><span class="fujita">藤田和之</span></span>，城所宏行，伊藤雄一．アナログパラパラデジタルマンガ，日本バーチャルリアリティ学会第17回大会 オーガナイズドセッション, 2012年9月．
                                </li>
                            </ol>
 						</div>
					</div>
				</div>
			</div>
			<!-- /.container -->
		</section>

		<p id="back-top">
			<a href="#top"><i class="fa fa-angle-up"></i></a>
		</p>
		<footer>
			<div class="container text-center">
				<p>Designed by <a href="http://moozthemes.com"><span>MOOZ</span>Themes.com</a></p>
			</div>
		</footer>

		<!-- Modal for portfolio item 1 -->
		<div class="modal fade" id="Modal-1" tabindex="-1" role="dialog" aria-labelledby="Modal-label-1">
			<div class="modal-dialog" role="document">
				<div class="modal-content">
					<div class="modal-header">
						<button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
						<h4 class="modal-title" id="Modal-label-1">Dean & Letter</h4>
					</div>
					<div class="modal-body">
						<img src="images/demo/portfolio-1.jpg" alt="img01" class="img-responsive" />
						<div class="modal-works"><span>Branding</span><span>Web Design</span></div>
						<p>Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe</p>
					</div>
					<div class="modal-footer">
						<button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
					</div>
				</div>
			</div>
		</div>

		<!-- Modal for portfolio item 2 -->
		<div class="modal fade" id="Modal-2" tabindex="-1" role="dialog" aria-labelledby="Modal-label-2">
			<div class="modal-dialog" role="document">
				<div class="modal-content">
					<div class="modal-header">
						<button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
						<h4 class="modal-title" id="Modal-label-2">Startup Framework</h4>
					</div>
					<div class="modal-body">
						<img src="images/demo/portfolio-2.jpg" alt="img01" class="img-responsive" />
						<div class="modal-works"><span>Branding</span><span>Web Design</span></div>
						<p>Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe</p>
					</div>
					<div class="modal-footer">
						<button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
					</div>
				</div>
			</div>
		</div>

		<!-- Modal for portfolio item 3 -->
		<div class="modal fade" id="Modal-3" tabindex="-1" role="dialog" aria-labelledby="Modal-label-3">
			<div class="modal-dialog" role="document">
				<div class="modal-content">
					<div class="modal-header">
						<button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
						<h4 class="modal-title" id="Modal-label-3">Lamp & Velvet</h4>
					</div>
					<div class="modal-body">
						<img src="images/demo/portfolio-3.jpg" alt="img01" class="img-responsive" />
						<div class="modal-works"><span>Branding</span><span>Web Design</span></div>
						<p>Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe</p>
					</div>
					<div class="modal-footer">
						<button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
					</div>
				</div>
			</div>
		</div>

		<!-- Modal for portfolio item 4 -->
		<div class="modal fade" id="Modal-4" tabindex="-1" role="dialog" aria-labelledby="Modal-label-4">
			<div class="modal-dialog" role="document">
				<div class="modal-content">
					<div class="modal-header">
						<button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
						<h4 class="modal-title" id="Modal-label-4">Smart Name</h4>
					</div>
					<div class="modal-body">
						<img src="images/demo/portfolio-4.jpg" alt="img01" class="img-responsive" />
						<div class="modal-works"><span>Branding</span><span>Web Design</span></div>
						<p>Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe</p>
					</div>
					<div class="modal-footer">
						<button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
					</div>
				</div>
			</div>
		</div>

		<!-- Modal for portfolio item 5 -->
		<div class="modal fade" id="Modal-5" tabindex="-1" role="dialog" aria-labelledby="Modal-label-5">
			<div class="modal-dialog" role="document">
				<div class="modal-content">
					<div class="modal-header">
						<button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
						<h4 class="modal-title" id="Modal-label-5">Fast People</h4>
					</div>
					<div class="modal-body">
						<img src="images/demo/portfolio-5.jpg" alt="img01" class="img-responsive" />
						<div class="modal-works"><span>Branding</span><span>Web Design</span></div>
						<p>Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe</p>
					</div>
					<div class="modal-footer">
						<button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
					</div>
				</div>
			</div>
		</div>

		<!-- Bootstrap core JavaScript
			================================================== -->
		<!-- Placed at the end of the document so the pages load faster -->
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
		<script src="js/bootstrap.min.js"></script>
		<script src="js/SmoothScroll.js"></script>
		<script src="js/theme-scripts.js"></script>
	</body>
</html>
